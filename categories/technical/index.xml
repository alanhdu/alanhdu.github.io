<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>technical on Amateur Hour</title>
    <link>https://alanhdu.github.io/categories/technical/</link>
    <description>Recent content in technical on Amateur Hour</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://alanhdu.github.io/categories/technical/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Intermediate Property-based Testing</title>
      <link>https://alanhdu.github.io/posts/2023-07-14-property-based-testing/</link>
      <pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2023-07-14-property-based-testing/</guid>
      <description>For such an effective way of ferreting out bugs in systems, property-based testing is drastically underused today. I&amp;rsquo;ve been using property-based testing for almost 10 years now, and in that time I&amp;rsquo;ve encountered maybe 3 other coworkers who also use property-based testing, even for systems that are ripe for it.
IMO, a big reason for this is that property-based testing has a &amp;ldquo;draw-the-owl&amp;rdquo; problem: most articles begin with fairly artificial examples (e.</description>
    </item>
    
    <item>
      <title>A Debugging Story</title>
      <link>https://alanhdu.github.io/posts/2023-03-12-debug-story/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2023-03-12-debug-story/</guid>
      <description>Debugging is a core skill for software engineers, but IMO it&amp;rsquo;s one that we often neglect. There&amp;rsquo;s a ton of advice on the internet on how to be a better systems designer, or how to solve algorithms challenges (albeit mostly for interviews), but very little on how to get better at debugging.
I wanted to help solve that by trying to document a real-world bug (https://github.com/HypothesisWorks/hypothesis/issues/3446 stumped me for quite some time.</description>
    </item>
    
    <item>
      <title>Evaluation in Deep Learning</title>
      <link>https://alanhdu.github.io/posts/2022-10-30-ml-robustness/</link>
      <pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2022-10-30-ml-robustness/</guid>
      <description>Over time, I&amp;rsquo;ve found that a lot of deep learning research is a mirage. While I&amp;rsquo;ve usually (but not always!) been able to reproduce the authors&#39; results on their chosen benchmarks, the performance gains often disappear if I choose other benchmarks to train/test against. This gets even worse when I try to apply their results to my problems, which is an extremely different data domain (surface electromyography, than the standard vision/text/speech that most people work with.</description>
    </item>
    
    <item>
      <title>Nice bounds on black box entropy estimators</title>
      <link>https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/</guid>
      <description>TLDR: A brief summary of a paper that I read recently about why it&amp;rsquo;s hard to estimate information entropy.
I recently came across a paper with some proofs of the number of samples you need to accurately estimate various information theoretic quantities in a &amp;ldquo;black box&amp;rdquo; way. Although I enjoyed the proofs, I found hard to understand as-written, and so I thought I&amp;rsquo;d post a more understandable explanation of the core results.</description>
    </item>
    
    <item>
      <title>Notes on Policy Gradients</title>
      <link>https://alanhdu.github.io/posts/2020-02-16-policy-gradients/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-02-16-policy-gradients/</guid>
      <description>TLDR: Some quick notes about policy gradient reinforcement learning algorithms.
Setup/Notation We usually think of reinforcement learning in terms of a Markovian decision process: our world consists of a set of states \(s \in S\) and possible actions \(a \in A\). At each time stamp, our agent can observe our current state \(s_t\), and then use some policy \(\pi(a |s)\) to choose some action \(a_t\). The world then updates according to some (usually unknown) transition probability \(\Pr[s_{t+1} | s_t, a_t]\) to a new state and we are given some reward \(\gamma^t R(s_t, a_t, s_{t+1})\), where \(0 is some arbitrary discounting factor.</description>
    </item>
    
    <item>
      <title>A Few Thoughts on the Lean Theorem Prover</title>
      <link>https://alanhdu.github.io/posts/2020-01-20-a-few-thoughts-on-lean/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-01-20-a-few-thoughts-on-lean/</guid>
      <description>TLDR: I tried out Lean, an automated theorem prover from Microsoft research and have some thoughts about it.
Now, probably like everyone with a background in both computers and mathematics, I&amp;rsquo;ve had this long-standing fantasy that paper proofs in mathematics will eventually be taken over by computer checked proofs. Although the philosophical core of mathematics is about rigorous proofs, actually doing rigorous axiomatic proofs is a giant pain, and so no one I know actually does them.</description>
    </item>
    
    <item>
      <title>Three Derivations of the Gaussian Distribution</title>
      <link>https://alanhdu.github.io/posts/2019-10-21-normal-distribution-derivation/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2019-10-21-normal-distribution-derivation/</guid>
      <description>This discussion is adapted from ET Jayne&amp;rsquo;s Probability Theory: A Logic of Science.
Preliminaries The Gaussian (or normal) distribution is a special distribution parametrized by mean \(\mu\) and variance \(\sigma^2\), with pdf:
\[p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x - \mu)^2} \]
The key fact about the Gaussian distribution (and the reason for its ubiquity) is that its pdf is the exponent of a quadratic function &amp;ndash; any pdf which is proportional to \(e^{-ax^2 + bx + c}\) will be a Gaussian distribution.</description>
    </item>
    
    <item>
      <title>Reflections on NIPS (Part 2)</title>
      <link>https://alanhdu.github.io/posts/2016-12-14-nips2/</link>
      <pubDate>Wed, 14 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2016-12-14-nips2/</guid>
      <description>One nice thing about going to NIPS is how intellectual stimulating it was. I&amp;rsquo;ve spent most of the past few months thinking about other things, so being immersed into the machine learning world again was definitely reinvigorating. Before the intellectual high fades, I thought I&amp;rsquo;d sketch out the most interesting puzzles that I&amp;rsquo;m thinking about thanks to NIPS.
Learning Structure Humans can learn from an incredibly small amount of data. For example, if I showed you a single picture of an Okapi, you could probably identify an Okapi if you ever saw one again.</description>
    </item>
    
    <item>
      <title>The Statistical Wasteland</title>
      <link>https://alanhdu.github.io/posts/2016-05-29-stats-syllabus/</link>
      <pubDate>Sun, 29 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2016-05-29-stats-syllabus/</guid>
      <description>TLDR: Columbia&amp;rsquo;s introductory statistics classes are horrible, and I propose a syllabus for a revamped intro to stats class.
In case it isn&amp;rsquo;t obvious, people suck at statistics. Even professional researchers mess up sometimes (and that&amp;rsquo;s not even touching the current replication crisis.
The amount of basic statistical ignorance among otherwise intelligent and informed people is astounding. I&amp;rsquo;ve lost track of how many times I need to point out that &amp;ldquo;correlation is not causation&amp;rdquo;, or &amp;ldquo;median and mean measure different things, especially in skewed data, or &amp;ldquo;statistically significant does not mean true&amp;rdquo;, or something equally basic.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation</title>
      <link>https://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</link>
      <pubDate>Sat, 05 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</guid>
      <description>TLDR I talk about a technique called automatic differentiation, going through a mathematical derivation before examining two different implementations: one in Rust and one in Python.
About a year ago, I read a blog post on automatic differentiation, a cool technique which automatically computes derivatives (and generalizations like gradients and Jacobians). That might not seem that interesting &amp;ndash; after all, we could just use finite differences to calculate derivatives:
\[\frac{df}{dx} \approx \frac{f(x + h) - f(x)}{h} \]</description>
    </item>
    
    <item>
      <title>A small shell in rust</title>
      <link>https://alanhdu.github.io/posts/2015-09-24-rust-shell/</link>
      <pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2015-09-24-rust-shell/</guid>
      <description>NOTE: You can see the source code for the shell on GitHub.
After turning in my first Operating Systems homework, I remembered why people invented high-level languages. C&amp;rsquo;s beautiful and powerful and really bare-metal, but it&amp;rsquo;s also insanely easy to shoot yourself with it. And because I haven&amp;rsquo;t programmed in C for 2 or 3 years, I probably made every stupid mistake imaginable. Accidentally returned a pointer to something allocated on the stack?</description>
    </item>
    
    <item>
      <title>Converging Random Variables</title>
      <link>https://alanhdu.github.io/posts/2015-05-06-random-variable-convergence/</link>
      <pubDate>Wed, 06 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2015-05-06-random-variable-convergence/</guid>
      <description>Note: In Dec 2019, I re-worked the exposition and expanded certain parts of this blog post.
In an introduction to probability, you&amp;rsquo;ll probably deal with lots of random variables (RVs). Like any other expression, we want to talk about the convergence of random variables too (e.g. convergence of the sample mean to population mean with the the law of large numbers). The problem is that there are three major definitions of random variable convergence:</description>
    </item>
    
  </channel>
</rss>
