<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Amateur Hour</title>
    <link>https://alanhdu.github.io/tags/machine-learning/</link>
    <description>Recent content in machine learning on Amateur Hour</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alanhdu.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Evaluation in Deep Learning</title>
      <link>https://alanhdu.github.io/posts/2022-10-30-ml-robustness/</link>
      <pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2022-10-30-ml-robustness/</guid>
      <description>Over time, I&amp;rsquo;ve found that a lot of deep learning research is a mirage. While I&amp;rsquo;ve usually (but not always!) been able to reproduce the authors&#39; results on their chosen benchmarks, the performance gains often disappear if I choose other benchmarks to train/test against. This gets even worse when I try to apply their results to my problems, which is an extremely different data domain (surface electromyography, than the standard vision/text/speech that most people work with.</description>
    </item>
    
    <item>
      <title>Nice bounds on black box entropy estimators</title>
      <link>https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/</guid>
      <description>TLDR: A brief summary of a paper that I read recently about why it&amp;rsquo;s hard to estimate information entropy.
I recently came across a paper with some proofs of the number of samples you need to accurately estimate various information theoretic quantities in a &amp;ldquo;black box&amp;rdquo; way. Although I enjoyed the proofs, I found hard to understand as-written, and so I thought I&amp;rsquo;d post a more understandable explanation of the core results.</description>
    </item>
    
    <item>
      <title>Notes on Policy Gradients</title>
      <link>https://alanhdu.github.io/posts/2020-02-16-policy-gradients/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-02-16-policy-gradients/</guid>
      <description>TLDR: Some quick notes about policy gradient reinforcement learning algorithms.
Setup/Notation We usually think of reinforcement learning in terms of a Markovian decision process: our world consists of a set of states \(s \in S\) and possible actions \(a \in A\). At each time stamp, our agent can observe our current state \(s_t\), and then use some policy \(\pi(a |s)\) to choose some action \(a_t\). The world then updates according to some (usually unknown) transition probability \(\Pr[s_{t+1} | s_t, a_t]\) to a new state and we are given some reward \(\gamma^t R(s_t, a_t, s_{t+1})\), where \(0 is some arbitrary discounting factor.</description>
    </item>
    
    <item>
      <title>Reflections on NIPS (Part 2)</title>
      <link>https://alanhdu.github.io/posts/2016-12-14-nips2/</link>
      <pubDate>Wed, 14 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2016-12-14-nips2/</guid>
      <description>One nice thing about going to NIPS is how intellectual stimulating it was. I&amp;rsquo;ve spent most of the past few months thinking about other things, so being immersed into the machine learning world again was definitely reinvigorating. Before the intellectual high fades, I thought I&amp;rsquo;d sketch out the most interesting puzzles that I&amp;rsquo;m thinking about thanks to NIPS.
Learning Structure Humans can learn from an incredibly small amount of data. For example, if I showed you a single picture of an Okapi, you could probably identify an Okapi if you ever saw one again.</description>
    </item>
    
    <item>
      <title>Reflections on NIPS (Part 1)</title>
      <link>https://alanhdu.github.io/posts/2016-12-13-nips1/</link>
      <pubDate>Tue, 13 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2016-12-13-nips1/</guid>
      <description>Last week, I went to Barcelona to attend NIPS 2016, or the Conference on Neural Information Processing Systems. NIPS is essentially a giant research conference on machine learning and neural networks, with a dash of cognitive science and neuroscience thrown-in. It was super interesting, so I thought I&amp;rsquo;d jot down some thoughts while the experience is still fresh in my mind.
First off, NIPS was an amazing experience! It&amp;rsquo;s one thing to read papers from Arxiv and follow some researchers&#39; blogs &amp;ndash; it&amp;rsquo;s totally different to actually meet researchers in person and interact with them.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation</title>
      <link>https://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</link>
      <pubDate>Sat, 05 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</guid>
      <description>TLDR I talk about a technique called automatic differentiation, going through a mathematical derivation before examining two different implementations: one in Rust and one in Python.
About a year ago, I read a blog post on automatic differentiation, a cool technique which automatically computes derivatives (and generalizations like gradients and Jacobians). That might not seem that interesting &amp;ndash; after all, we could just use finite differences to calculate derivatives:
\[\frac{df}{dx} \approx \frac{f(x + h) - f(x)}{h} \]</description>
    </item>
    
  </channel>
</rss>
