<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on Amateur Hour</title>
    <link>https://alanhdu.github.io/tags/math/</link>
    <description>Recent content in math on Amateur Hour</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Mar 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://alanhdu.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Nice bounds on black box entropy estimators</title>
      <link>https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/</guid>
      <description>TLDR: A brief summary of a paper that I read recently about why it&amp;rsquo;s hard to estimate information entropy.
I recently came across a paper with some proofs of the number of samples you need to accurately estimate various information theoretic quantities in a &amp;ldquo;black box&amp;rdquo; way. Although I enjoyed the proofs, I found hard to understand as-written, and so I thought I&amp;rsquo;d post a more understandable explanation of the core results.</description>
    </item>
    
    <item>
      <title>Notes on Policy Gradients</title>
      <link>https://alanhdu.github.io/posts/2020-02-16-policy-gradients/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-02-16-policy-gradients/</guid>
      <description>TLDR: Some quick notes about policy gradient reinforcement learning algorithms.
Setup/Notation We usually think of reinforcement learning in terms of a Markovian decision process: our world consists of a set of states \(s \in S\) and possible actions \(a \in A\). At each time stamp, our agent can observe our current state \(s_t\), and then use some policy \(\pi(a |s)\) to choose some action \(a_t\). The world then updates according to some (usually unknown) transition probability \(\Pr[s_{t+1} | s_t, a_t]\) to a new state and we are given some reward \(\gamma^t R(s_t, a_t, s_{t+1})\), where \(0 is some arbitrary discounting factor.</description>
    </item>
    
    <item>
      <title>A Few Thoughts on the Lean Theorem Prover</title>
      <link>https://alanhdu.github.io/posts/2020-01-20-a-few-thoughts-on-lean/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2020-01-20-a-few-thoughts-on-lean/</guid>
      <description>TLDR: I tried out Lean, an automated theorem prover from Microsoft research and have some thoughts about it.
Now, probably like everyone with a background in both computers and mathematics, I&amp;rsquo;ve had this long-standing fantasy that paper proofs in mathematics will eventually be taken over by computer checked proofs. Although the philosophical core of mathematics is about rigorous proofs, actually doing rigorous axiomatic proofs is a giant pain, and so no one I know actually does them.</description>
    </item>
    
    <item>
      <title>Three Derivations of the Gaussian Distribution</title>
      <link>https://alanhdu.github.io/posts/2019-10-21-normal-distribution-derivation/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2019-10-21-normal-distribution-derivation/</guid>
      <description>This discussion is adapted from ET Jayne&amp;rsquo;s Probability Theory: A Logic of Science.
Preliminaries The Gaussian (or normal) distribution is a special distribution parametrized by mean \(\mu\) and variance \(\sigma^2\), with pdf:
\[p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x - \mu)^2} \]
The key fact about the Gaussian distribution (and the reason for its ubiquity) is that its pdf is the exponent of a quadratic function &amp;ndash; any pdf which is proportional to \(e^{-ax^2 + bx + c}\) will be a Gaussian distribution.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation</title>
      <link>https://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</link>
      <pubDate>Sat, 05 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</guid>
      <description>TLDR I talk about a technique called automatic differentiation, going through a mathematical derivation before examining two different implementations: one in Rust and one in Python.
About a year ago, I read a blog post on automatic differentiation, a cool technique which automatically computes derivatives (and generalizations like gradients and Jacobians). That might not seem that interesting &amp;ndash; after all, we could just use finite differences to calculate derivatives:
\[\frac{df}{dx} \approx \frac{f(x + h) - f(x)}{h} \]</description>
    </item>
    
    <item>
      <title>Converging Random Variables</title>
      <link>https://alanhdu.github.io/posts/2015-05-06-random-variable-convergence/</link>
      <pubDate>Wed, 06 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://alanhdu.github.io/posts/2015-05-06-random-variable-convergence/</guid>
      <description>Note: In Dec 2019, I re-worked the exposition and expanded certain parts of this blog post.
In an introduction to probability, you&amp;rsquo;ll probably deal with lots of random variables (RVs). Like any other expression, we want to talk about the convergence of random variables too (e.g. convergence of the sample mean to population mean with the the law of large numbers). The problem is that there are three major definitions of random variable convergence:</description>
    </item>
    
  </channel>
</rss>
