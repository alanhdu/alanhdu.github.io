<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Amateur Hour</title>
    <link>http://alanhdu.github.io/posts/</link>
    <description>Recent content in Posts on Amateur Hour</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://alanhdu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Three Derivations of the Gaussian Distribution</title>
      <link>http://alanhdu.github.io/posts/2019-10-21-normal-distribution-derivation/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2019-10-21-normal-distribution-derivation/</guid>
      <description>This discussion is adapted from ET Jayne&#39;s Probability Theory: A Logic of Science.
Preliminaries The Gaussian (or normal) distribution is a special distribution parametrized by mean \(\mu\) and variance \(\sigma^2\), with pdf:
\[ p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x - \mu)^2} \]
The key fact about the Gaussian distribution (and the reason for its ubiquity) is that its pdf is exponent of a quadratic function -- any pdf which is proportional to \(e^{-ax^2 + bx + c}\) will be a Gaussian distribution.</description>
    </item>
    
    <item>
      <title>A Fresh Start</title>
      <link>http://alanhdu.github.io/posts/2019-10-20-new-blog/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2019-10-20-new-blog/</guid>
      <description>Since the last time I blogged, I&amp;rsquo;ve graduated from Columbia and have thus lost access to my previous blog setup. I was content to ignore my blog entirely and let it languish, but I was recently asked to post some notes on the Gaussian distribution and decided to resurrect my blog (the person was sort of drunk and might not have been serious, but that&amp;rsquo;s ok).
Unfortunately, I could not find the backups of all of my previous posts.</description>
    </item>
    
    <item>
      <title>Reflections on NIPS (Part 2)</title>
      <link>http://alanhdu.github.io/posts/2016-12-14-nips2/</link>
      <pubDate>Wed, 14 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2016-12-14-nips2/</guid>
      <description>One nice thing about going to NIPS is how intellectual stimulating it was. I&amp;rsquo;ve spent most of the past few months thinking about other things, so being immersed into the machine learning world again was definitely reinvigorating. Before the intellectual high fades, I thought I&amp;rsquo;d sketch out the most interesting puzzles that I&amp;rsquo;m thinking about thanks to NIPS.
Learning Structure Humans can learn from an incredibly small amount of data.</description>
    </item>
    
    <item>
      <title>Reflections on NIPS (Part 1)</title>
      <link>http://alanhdu.github.io/posts/2016-12-13-nips1/</link>
      <pubDate>Tue, 13 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2016-12-13-nips1/</guid>
      <description>Last week, I went to Barcelona to attend NIPS 2016, or the Conference on Neural Information Processing Systems. NIPS is essentially a giant research conference on machine learning and neural networks, with a dash of cognitive science and neuroscience thrown-in. It was super interesting, so I thought I&amp;rsquo;d jot down some thoughts while the experience is still fresh in my mind.
First off, NIPS was an amazing experience! It&amp;rsquo;s one thing to read papers from Arxiv and follow some researchers&amp;rsquo; blogs &amp;ndash; it&amp;rsquo;s totally different to actually meet researchers in person and interact with them.</description>
    </item>
    
    <item>
      <title>The Statistical Wasteland</title>
      <link>http://alanhdu.github.io/posts/2016-05-29-stats-syllabus/</link>
      <pubDate>Sun, 29 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2016-05-29-stats-syllabus/</guid>
      <description>TLDR: Columbia&#39;s introductory statistics classes are horrible, and I propose a syllabus for a revamped intro to stats class.
In case it isn&#39;t obvious, people suck at statistics. Even professional researchers mess up sometimes (and that&#39;s not even touching the current replication crisis.
The amount of basic statistical ignorance among otherwise intelligent and informed people is astounding. I&#39;ve lost track of how many times I need to point out that &amp;quot;correlation is not causation&amp;quot;, or &amp;quot;median and mean measure different things, especially in skewed data, or &amp;quot;statistically significant does not mean true&amp;quot;, or something equally basic.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation</title>
      <link>http://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</link>
      <pubDate>Sat, 05 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2015-12-05-automatic-differentiation/</guid>
      <description>TLDR I talk about a technique called automatic differentiation, going through a mathematical derivation before examining two different implementations: one in Rust and one in Python.
About a year ago, I read a blog post on automatic differentiation, a cool technique which automatically computes derivatives (and generalizations like gradients and Jacobians). That might not seem that interesting -- after all, we could just use finite differences to calculate derivatives:
\[\frac{df}{dx} \approx \frac{f(x + h) - f(x)}{h}\]</description>
    </item>
    
    <item>
      <title>A small shell in rust</title>
      <link>http://alanhdu.github.io/posts/2015-09-24-rust-shell/</link>
      <pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>http://alanhdu.github.io/posts/2015-09-24-rust-shell/</guid>
      <description>NOTE: You can see the source code for the shell on GitHub.
After turning in my first Operating Systems homework, I remembered why people invented high-level languages. C&amp;rsquo;s beautiful and powerful and really bare-metal, but it&amp;rsquo;s also insanely easy to shoot yourself with it. And because I haven&amp;rsquo;t programmed in C for 2 or 3 years, I probably made every stupid mistake imaginable. Accidentally returned a pointer to something allocated on the stack?</description>
    </item>
    
  </channel>
</rss>