<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1" /><title> What I&#39;ve Been Reading (Aug 2022) &mdash; Amateur Hour </title>
  <meta property="og:title" content="What I&#39;ve Been Reading (Aug 2022)" />
<meta property="og:description" content="a short log of things I read last month." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://alanhdu.github.io/posts/2022-09-04-what-im-reading/" />
<meta property="article:published_time" content="2022-09-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-09-04T00:00:00+00:00" />

	<link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/katex.min.css" />
<script defer type="text/javascript" src="https://alanhdu.github.io/js/katex.min.js"> </script>
<script defer type="text/javascript" src="https://alanhdu.github.io/js/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement(document.body, {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false}
    ]
  });
});
</script>
<link href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.min.js" integrity="sha384-Ep9Es0VCjVn9dFeaN2uQxgGcGmG+pfZ4eBaHxUpxXDORrrVACZVOpywyzvFRGbmv" crossorigin="anonymous"></script>


  <link rel="apple-touch-icon" sizes="180x180" href="https://alanhdu.github.io/favicon//apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="https://alanhdu.github.io/favicon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="https://alanhdu.github.io/favicon/favicon-16x16.png"/ >
</head>

  <body>
    <div class="container wrapper">
      <div class="header">
	<h1 class="site-title">Amateur Hour</h1>
	<nav class="nav">
		<ul class="flat">
      <li><a href="https://alanhdu.github.io/">Home</a></li>
      <li><a href="https://alanhdu.github.io/about">About</a></li>
      <li><a href="https://alanhdu.github.io/categories">Categories</a></li>
      <li><a href="https://alanhdu.github.io/tags">Tags</a></li>
      <li><a href="https://alanhdu.github.io/index.xml">RSS Feed</a></li>
    </ul>
  </nav>
</div>

      

<div class="post-header">
  <h2 class="title">What I&#39;ve Been Reading (Aug 2022)</h2>
  <strong class="description">Or a short log of things I read last month.</strong>
  <div>
    <span class="date">Sep 4, 2022</span>.
    Filed under
    
    <span class="category"> <a href="https://alanhdu.github.io/categories/musings">Musings</a></span>
  </div>
  Tags:
    
    <a href="https://alanhdu.github.io/tags/reading">reading</a>
</div>

<article class="markdown"><aside><strong> Table of Contents </strong> <nav id="TableOfContents">
  <ul>
    <li><a href="#books">Books</a>
      <ul>
        <li><a href="#a-thousand-acres">A Thousand Acres</a></li>
      </ul>
    </li>
    <li><a href="#articles--blog-posts">Articles / Blog Posts</a></li>
    <li><a href="#papers">Papers</a>
      <ul>
        <li><a href="#birds-and-frogs">Birds and Frogs</a></li>
        <li><a href="#photon-a-fast-query-engine-for-lakehouse-systems">Photon: A Fast Query Engine for Lakehouse Systems</a></li>
        <li><a href="#fixes-that-fail-self-defeating-improvements-in-machine-learning-systems">Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems</a></li>
        <li><a href="#plor-general-transactions-with-predictable-low-tail-latency">Plor: General Transactions with Predictable, Low Tail Latency</a></li>
        <li><a href="#hint-a-hierarchical-index-for-intervals-in-main-memory">HINT: A Hierarchical Index for Intervals in Main Memory</a></li>
        <li><a href="#finding-real-bugs-in-big-programs-with-incorrectness-logic">Finding real bugs in big programs with incorrectness logic</a></li>
        <li><a href="#decentralized-society-finding-web3s-soul">Decentralized Society: Finding Web3â€™s Soul</a></li>
        <li><a href="#enabling-the-next-generation-of-multi-region-applications-with-cockroachdb">Enabling the Next Generation of Multi-Region Applications with CockroachDB</a></li>
        <li><a href="#formality-considered-harmful-experiences-emerging-themes-and-directions-on-the-use-of-formal-representations-in-interactive-systems">Formality Considered Harmful: Experiences, Emerging Themes, and Directions on the Use of Formal Representations in Interactive Systems</a></li>
        <li><a href="#relational-e-matching">Relational E-matching</a></li>
        <li><a href="#a-tree-clock-data-structure-for-causal-orderings-in-concurrent-executions">A tree clock data structure for causal orderings in concurrent executions</a></li>
        <li><a href="#handoff-strategies-in-settings-with-high-consequences-for-failure-lessons-for-health-care-operations">Handoff strategies in settings with high consequences for failure: lessons for health care operations</a></li>
        <li><a href="#potential-outcome-and-directed-acyclic-graph-approaches-to-causality-relevance-for-empirical-practice-in-economics">Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics</a></li>
        <li><a href="#understanding-and-exploiting-optimal-function-inlining">Understanding and exploiting optimal function inlining</a></li>
      </ul>
    </li>
  </ul>
</nav> </aside>
  <h2 id="books">Books</h2>
<h3 id="a-thousand-acres">A Thousand Acres</h3>
<p><a href="https://www.amazon.com/Thousand-Acres-Novel-Jane-Smiley/dp/1400033837">https://www.amazon.com/Thousand-Acres-Novel-Jane-Smiley/dp/1400033837</a></p>
<p><em>A Thousand Acres</em> basically an adaptation of King Lear, set in rural
Iowa in a rural farming community. It&rsquo;s a deliberate reinterpretation,
portraying Larry / King Lear much less sympathetically (and trying to
humanize the &ldquo;evil&rdquo; daughters Ginny / Goneril and Rose / Regan). The
ending is also fairly different (although still quite bleak).</p>
<p>Overall, I enjoyed the book &ndash; it was well-written and engaging, but
there was nothing that made me really fall in love with it. I <em>did</em>
learn a surprising amount about farming though (although I question how
accurate the depiction of rural life was, given that the author doesn&rsquo;t
seem to be from a farming background themselves).  For instance, I had
no idea that you needed to install drain tiles (essentially underground
pipes that drain ground water) into fields to allow heavy machinery to
work.</p>
<p>One interesting thing about the experience is that when I read the book
summary on the book (which references &ldquo;dark truths&rdquo; about the family
that are brought to light), I immediately jumped to the idea that &ldquo;the
father (King Lear) sexually abused his daughters&rdquo; (although I wasn&rsquo;t
certain whether Cordelia or Goneril + Regan were abused). It turns out I
was correct (Goneril + Regan were raped in this telling), but I&rsquo;m not
entirely sure what this says about me or our literary culture
(especially since <em>A Thousand Acres</em> is from 1991 and not particularly
contemporary).</p>
<h2 id="articles--blog-posts">Articles / Blog Posts</h2>
<ul>
<li><a href="https://americanaffairsjournal.org/2021/05/inside-operation-warp-speed-a-new-model-for-industrial-policy/">Inside Operation Warp Speed</a>:
Operation Warp Speed was the Federal government project for developing,
manufacturing, and distributing Covid-19 vaccines. Obviously, it was a huge
success on the first two: vaccines were developed, tested, approved, and
manufactured in record times. Although the article did an ok job describing
how OWS happened, but I didn&rsquo;t find the analysis for <em>why</em> OWS to be super
novel &ndash; some of that is because I was already familiar with some of the
ideas (like <a href="https://www.worksinprogress.co/issue/buyers-of-first-resort/">buyers of first
resort</a> or the
<a href="https://www.ainventures.com/post/the-valley-of-death">valley of death</a>), but
some of it because the article really didn&rsquo;t go into enough detail on some
crucial parts. In particular it talks about the &ldquo;scale up&rdquo; problem in
American R&amp;D work, but never actually talks about <em>how</em> OWS scaled up other
than some vague references to &ldquo;supply chain mapping&rdquo;. Like, what does that
actually entail? How do you do a good job mapping the supply chain and where
does it go wrong? What happens if you don&rsquo;t do supply chain mapping? Despite
these gripes, I thought the article did a good job positioning OWS as a
uniquely successful triumph that should be studied and learned from.</li>
<li><a href="https://matklad.github.io/2022/04/25/why-lsp.html">Why LSP</a> &ndash; this
is a nice perspective on the rise of LSP and how IDE-like features
become widespread from someone who actually writes IDEs for a living. The
the article is a little poorly worded, but The
article basically argues that the common story for why LSPs are great
(saving work by turning an <span class="math inline">\(O(m \cdot n)\)</span> problem into a <span class="math inline">\(O(m) + O(n)\)</span>
problem) isn&rsquo;t quite correct, since the glue code between language servers
and editors is pretty trivial. Instead, the article argues that by creating a
functioning LSP for C# and TypeScript along with the VSCode editor to support
it, Microsoft created a <em>new</em> equilibrium where people <em>expected</em> languages
to implement LSPs (to hook into VSCode) while also expecting editors to hook
into LSPs (now that VSCode offered those features).</li>
<li><a href="https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html">Three Architectures for Responsive
IDEs</a>
&ndash; a nice description about how IDEs work and how they intersect with
language design. I always vaguely knew that certain langugage features
are &ldquo;hard&rdquo; to support properly in IDEs, but it was nice to see
specifics about how some language features (like Rust&rsquo;s macros and scoping
rules) can make even simple things like name resolution difficult.
This is the sort of content that I wished was circulated more widely in
compiler classes &ndash; while it might not be important for the language itself,
I think good IDE support is pretty much table-stakes for any new language
ecosystem.</li>
<li><a href="https://www.nytimes.com/2022/08/03/magazine/claremont-institute-conservative.html">How the Claremont Institute Became a Nerve Center of the American Right</a>:
I had vaguely heard of the Claremont Institute as the &ldquo;intellectual
Trumpist&rdquo; center before, but this article gave me a little more color on who
exactly they were, what they believe in, and how prominent they really are. I
don&rsquo;t have too many specific thoughts here (I try not to give judgements
based on single secondary sources), but the article did a good job pointing
out where to look if I ever want to dig in more.</li>
<li><a href="https://www.datadoghq.com/blog/engineering/introducing-husky">Introducing Husky</a>:
Husky is Datadog&rsquo;s 3rd generation event store (which I think is just a very
wide column store with, where some fields are very sparse). The architecture
seems pretty straightforward, and I can definitely see the design influence
of other &ldquo;cloud-native disaggregated&rdquo; databases like Snowflake (e.g.
leveraging a blob store like S3 for the durability guarantees, etc). There&rsquo;s
not a whole lot of detail, but it&rsquo;s an interesting data point about the rise
of these &ldquo;disaggregated&rdquo; cloud-native service designs.</li>
<li><a href="https://every.to/p/what-i-miss-about-working-at-stripe">What I Miss about Working at Stripe</a>:
This is basically a love-letter to (paraphrasing) &ldquo;workplaces where
people really care&rdquo; &ndash; that is places where people really buy into the core
mission and work with both high intensity and a high quality bar. I didn&rsquo;t
quite get the same experience interning at Stripe (I remember my internship
being fairly relaxed), but I recognize part of the described Stripe culture
and definitely experienced something similar at CTRL-labs). I&rsquo;m of two minds
here &ndash; I think these environments can be incredibly rewarding + fulfilling,
and so am definitely sympathetic to the nostalgia. At the same time, I&rsquo;ve
definitely seen how this can be incredibly unhealthy for people, even when
the organization&rsquo;s not deliberately exploitative. I haven&rsquo;t quite thought
through how to distinguish &ldquo;healthy passion + engagement&rdquo; vs &ldquo;unhealthy
obsession&rdquo;.</li>
<li><a href="https://www.nytimes.com/interactive/2022/07/27/magazine/barbados-climate-debt-mia-mottley.html">The Barbados Rebellion</a>:
This feature is specifically about Mia Mottley (the prime minister of
Barbados) and her efforts to create a climate financing system, but is more
broadly a story about climate change&rsquo;s effect on poorer nations of the world.
I knew abstractly that climate change is a disaster for poor island
countries, but it was definitely much more evocative with all the details. I
also very much appreciated the nitty-gritty about IMF internal politics
(adapting to new climate change paradigms but probably not fast enough),
international banking, and government negotiations (hurricane clauses!).</li>
<li><a href="https://arnoldkling.substack.com/p/what-should-gdp-measure-729">What should GDP measure</a>:
This was a short article that helped me clarify how to think about GDP. I
used to think of GDP as essentially an easier way of calculating GDI (Gross
Domestic Income, or the total income everyone had), but Kling adds the
perspective that GDP measures &ldquo;the extent of the market&rdquo;. I think this new
perspective is quite useful about thinking through what GDP is <em>not</em> (do we
want everything to be on the market and so included in GDP?).</li>
<li><a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">A whirlwhind introduction to dataflow graphs</a>:
This is an article that I wish I had thought of independently, about
a simple dataflow mental model to reason about instruction-level
parallelism. I think I knew all the &ldquo;facts&rdquo; needed to do this analysis
before-hand, but it was still instructive to see it all put together
and used to analyze some actual examples.</li>
</ul>
<h2 id="papers">Papers</h2>
<h3 id="birds-and-frogs">Birds and Frogs</h3>
<p><a href="https://www.ams.org/notices/200902/rtx090200212p.pdf">https://www.ams.org/notices/200902/rtx090200212p.pdf</a></p>
<p>This was a nice fun paper contrasting two different &ldquo;types&rdquo; of thinkers:
&ldquo;frogs&rdquo; who get down-and-dirty with very specific problems and &ldquo;birds&rdquo; who soar
above and try to connect different areas of mathematics together. Dyson goes
through a series of famous mathematicians/physicists trying to classify them as
either birds and frogs, arguing that both are necessary for forward progress. I
found this paper fun and breezy to read (with many historical tidbits I wasn&rsquo;t
familiar with like von Neumann&rsquo;s &ldquo;warmed up soup&rdquo; lecture)</p>
<h3 id="photon-a-fast-query-engine-for-lakehouse-systems">Photon: A Fast Query Engine for Lakehouse Systems</h3>
<p><a href="https://doi.org/10.1145/3514221.352605410.1145/3514221.3526054">https://doi.org/10.1145/3514221.352605410.1145/3514221.3526054</a></p>
<p>This paper describes Databricks' new (proprietary) query engine for Apache
Spark built for &ldquo;lakehouse&rdquo; architectures (architecture where queries executed
against &ldquo;raw&rdquo; data (e.g. Parquet files on S3) that haven&rsquo;t been cleaned,
indexed, or curated).</p>
<p>This was definitely more of an engineering-oriented paper &ndash; I don&rsquo;t think
there was anything super novel on the design side (this seemed like a fairly
&ldquo;standard&rdquo; C++ vectorized query engine with microadapatvity a la
<a href="https://dl.acm.org/doi/abs/10.1145/2463676.2465292">Vectorize</a>). The
interesting meat in the paper was about the actual engineering experience
things like &ldquo;how to get the JVM and C++ memory managment to play well?&rdquo; or &ldquo;how
to ensure compatibility with existing Spark operators?&rdquo; or &ldquo;what was our testing
strategy&rdquo; or &ldquo;what was our testing strategy?&rdquo;.</p>
<p>The authors' arguments about why they chose a vectorized query engine instead
of a query compiler was particularly interesting since Databricks has already
built a <a href="https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">Spark query compiler</a>
before. Some of the tradeoffs are pretty similar to the ones noted in <a href="https://dl.acm.org/doi/abs/10.14778/3275366.3284966">Kersten et
al</a>&rsquo;s bakeoff, namely
that:</p>
<ul>
<li>When the virtual call overhead is too much, it&rsquo;s relatively easy to
build specialized &ldquo;fused&rdquo; operators specific to that use-case.</li>
<li>Vectorized execution engines are easier to debug, because you can use
&ldquo;standard&rdquo; tooling for C++ instead of having to manually inject
debugging information into runtime generated code.</li>
<li>Vectorized execution engines are easier to instrument + monitor, since
(virtual) function calls + stack frames are preserved (instead of
being all inlined away in a compiled query) and you can easily obtain
&ldquo;operator-at-a-time&rdquo; statistics.</li>
<li>Vectorized execution engines are easier to make adaptive, since you don&rsquo;t
need to juggle multiple compiled queries for different data characteristics.</li>
</ul>
<p>I do wonder how true these claims would be if there was somebody w/ expertise
in JIT compilation (a la V8 or Graal), since they have to deal with pretty much
all of these issues. I always assumed that this kind of thing was deep black
magic, but maybe that&rsquo;s just a measure of my own ignorance rather than any
objective sense of complexity?</p>
<p>Although I thought these kinds of engineering considerations were really
well written + explained, I found the experimental evaluation to be less
satisfying &ndash; Databricks mostly benchmarked against fairly
artificial micro-benchmarks. It would&rsquo;ve been nice to dig into the TPC-H and
TPC-DS results a little more and pinpoint (1) where time is still being spent
in Photon, (2) how things stack up against an old school
&ldquo;data warehouse&rdquo; type architecture, and (3) how crucial the adaptive runtime is
to the speed-ups. My impression was that while the &ldquo;sparsity adaptation&rdquo; (e.g.
adapting based on how much data is NULL) is super important, I didn&rsquo;t get a
good sense of how common their &ldquo;string&rdquo; happy-paths were (e.g. ASCII-only vs UTF-8, or UUID specialized
query paths), or whether there were other heuristics to store.</p>
<h3 id="fixes-that-fail-self-defeating-improvements-in-machine-learning-systems">Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems</h3>
<p><a href="https://proceedings.neurips.cc/paper/2021/hash/619427579e7b067421f6aa89d4a8990c-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/619427579e7b067421f6aa89d4a8990c-Abstract.html</a></p>
<p>I would call this a &ldquo;Machine Learning Engineering Vision&rdquo; paper &ndash; I
think it did a good job identifying a set of challenges for building
real-world machine learning systems, but it didn&rsquo;t go much further than
that.</p>
<p>The problem setup is that, in practice, many machine learning systems are <em>not</em>
trained end-to-end. Instead, you often have multiple independent machine
learning algorithms that depend on each other&rsquo;s outputs (e.g. a generic
&ldquo;embedding&rdquo; model with different feature heads that are trained separately).</p>
<p>The authors introduce a decomposition of the &ldquo;system-level&rdquo; error into three
parts:</p>
<ul>
<li>Upstream error: how much the upstream model diverges from the &ldquo;optimal&rdquo; model</li>
<li>Downstream approximation error (I would call this downstream misspecifcation
error): how much the optimal downstream model of that type diverges from the
globally optimal downstream model</li>
<li>Downstream estimation error:  how much the downstream model diverges from its
optimal form.</li>
</ul>
<p>The basic problem is that improving the upstream error can actually make
these errors <em>worse</em>, even after retraining the downstream model: for
instance, the upstream model improvement might not be exploitable by the
downstream model architecture (increasing the downstream approximation
error), or the upstream model&rsquo;s loss function diverge from the
downstream model&rsquo;s loss function (increasing the upstream error).</p>
<p>They end with a self-driving car example, which I found a little
artificial (although I&rsquo;m also not familiar with the domain). They didn&rsquo;t
really introduce any solutions to the challenges presented here, and was
more of a call-to-action.</p>
<h3 id="plor-general-transactions-with-predictable-low-tail-latency">Plor: General Transactions with Predictable, Low Tail Latency</h3>
<p><a href="https://doi.org/10.1145/3514221.3517879">https://doi.org/10.1145/3514221.3517879</a></p>
<p>This paper introduces a new concurrency control protocol for databases
with an emphasis on lowering tail latency. Traditional 2PL can have poor
throughput in low-contention scenarios due to lock overhead, while OCC
can have extremely poor tail latencies since &ldquo;unlucky&rdquo; transactions can
just endlessly retry. The paper introduces a &ldquo;Pessimistic Locking
Optimistic Reading&rdquo; protocol that tries to blend the two ideas.</p>
<p>I found the paper&rsquo;s description pretty hard to follow &ndash; I don&rsquo;t know
what it was, but I had to go through Section 4.1 three or four times
before I felt like I really understand the protocol. I think of PLOR as
basically OCC, except we always try to abort the <em>younger</em> transaction
(allowing the older transaction to make progress). They do this by (1)
using locks to eagerly resolve write-write conflicts during the initial
OCC &ldquo;read&rdquo; phase (using WOUND WAIT) and (2) recording <em>which</em> transactions
are reading/writing to each row so we can know which transactions to
abort when checking for conflicts.</p>
<p>Although this was an interesting idea, I didn&rsquo;t find this paper
convincing on a number of levels:</p>
<ul>
<li>I don&rsquo;t think the authors did a good job establishing what the &ldquo;target
latency&rdquo; should be for any actual set of applications. In the evaluation,
even the &ldquo;high-latency&rdquo; protocols had P999 latencies of 5-ish ms which&hellip;
doesn&rsquo;t seem that high (a single spinning disk seek can easily have that
latency)? Especially in the context of a distributed database with
potentially multiple network round trips, how important are these gains in
practice?
<ul>
<li>Their UNDO / REDO logging was implemented using Optane which might be part
of this? This seems like a weird choice to me &ndash; even at the time of
publication I wasn&rsquo;t aware of any real-world large-scale deployment of
Optane, let alone now when Intel has officially retired Optane.</li>
</ul>
</li>
<li>At the protocol level, they didn&rsquo;t do the work of integrating PLOR with MVCC.
This seems like a weird hole on two levels:
<ul>
<li>AFAIK every new production database engine has used some form of MVCC over
the last 10 years, so it seems like a natural thing to do.</li>
<li>MVCC guarantees that read-only transactions don&rsquo;t conflict with anything,
which could potentially actualy improve PLOR throughput (reducing the
degree to which &ldquo;reads execute writes&rdquo; by registering themselves for each
row). The authors sort of sneak by this by actually executing read-only
transactions using pure OCC and only falling back to PLOR after 3 aborts,
but MVCC seems more principled.</li>
</ul>
</li>
</ul>
<h3 id="hint-a-hierarchical-index-for-intervals-in-main-memory">HINT: A Hierarchical Index for Intervals in Main Memory</h3>
<p><a href="https://doi.org/10.1145/3514221.3517873">https://doi.org/10.1145/3514221.3517873</a></p>
<p>This was a nice, sweet paper about intersection queries for contiguous ranges
&ndash; basically, given a set of <span class="math inline">\([a, b]\)</span> intervals, we want to find all intervals
that intersect some <span class="math inline">\([l, u]\)</span> query.</p>
<p>The image below is the key idea: given a <span class="math inline">\([0, 2^m]\)</span> domain,
we can recursively (and hierarchically) partition up your space into
&ldquo;subintervals&rdquo;. Each subinterval <span class="math inline">\(P_{l, f}\)</span> stores the set of intervals whose
integer bounding box is fully contained within <span class="math inline">\([f \cdot 2^{m-l}, (f + 1) \cdot 2^{m-l}]\)</span>. Note that any interval we want to store is going to be
contained by at most 2 subintervals per level.</p>
<p><img src="hint.png" alt="HINT partitioning diagram"></p>
<p>To execute queries, we just loop through all &ldquo;subpartitions&rdquo; that
intersect our query interval and search their subcomponents. In effect,</p>
<p>The rest of the paper defines how to generalize this scheme to arbitrary
domains (essentially define a remapping function into <span class="math inline">\([0, 2^m-1]\)</span>), how
to define the optimal <span class="math inline">\(m\)</span> (trading off # of subintervals to visit with
the # of stored intervals per subinterval you need to handle), and some
implementation optimizations.</p>
<p>I didn&rsquo;t read the performance evaluations super closely since I&rsquo;m not
familiar with the evaluation norms here, but the evaluation seemed
detailed, plausible (a mix of real-world and synthetic data on a variety
of conditions), and impressive (much faster queries than competitors at
the cost of slower updates). Overall, I thought this was a neat idea!</p>
<h3 id="finding-real-bugs-in-big-programs-with-incorrectness-logic">Finding real bugs in big programs with incorrectness logic</h3>
<p><a href="https://doi.org/10.1145/3527325">https://doi.org/10.1145/3527325</a></p>
<p>This paper goes through some details about actually applying
incorrectness logic into real-world static analysis.</p>
<p>Incorrectness logic is the sort of &ldquo;dual&rdquo; with Hoare logic &ndash; in Hoare
logic, you have <code>{P} C {Q}</code> which roughly means &ldquo;if P is true and then
code C executes, then Q <em>must</em> be true&rdquo;, while in Incorrectness logic you
have <code>[P] C [Q]</code> which means &ldquo;if P is true and C executes, then Q <em>can</em>
be true&rdquo;. Hoare logic is often used for formal verification (e.g. Q is
roughly &ldquo;implemented correctly&rdquo;), but the authors argue that
incorrectness logic is more natural for bug-finding static analysis (Q
is something like &ldquo;is a bug&rdquo;).</p>
<p>I don&rsquo;t understand the type thoery enough to fully grok the details of
the analysis, but it roughly follows the shape of the &ldquo;compositional
analysis&rdquo; described
<a href="https://cacm.acm.org/magazines/2019/8/238344-scaling-static-analyses-at-facebook/fulltext">here</a>.
I think the interesting part from a non-specialist was the distinction between
&ldquo;latent bugs&rdquo; and &ldquo;manifest bugs&rdquo;. Suppose you find a bug that &ldquo;could&rdquo; occur
based on your analysis &ndash; who do you know whether this is a &ldquo;real&rdquo; bug or a
missing pre-condition you didn&rsquo;t know about (e.g. an undocumented assumption
that a pointer is not null)? The authors argue that the &ldquo;right&rdquo; answer is based
on <em>social</em> reasons (what do software engineers expect in practice?) and  come
up with some heuristics here.</p>
<h3 id="decentralized-society-finding-web3s-soul">Decentralized Society: Finding Web3â€™s Soul</h3>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4105763">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4105763</a></p>
<p>This is a Web3 paper introducing the idea of &ldquo;soulbound&rdquo; tokens. A &ldquo;soul&rdquo; is
basically a single wallet / digital entity, and a soulbound token is a token
that can&rsquo;t be transferred (only given out / revoked). The big idea is to use
soulbound tokens to represent community identities (for instance, a school
&ldquo;soul&rdquo; could issue soulbound tokens to its alumni) &ndash; if there are enough souls
with overlapping communities, this could plausibly let you capture lots of
social information and expose it to smart contracts (e.g. to distinguish real
humans from bots).</p>
<p>I thought this was definitely a clever idea that opens up some interesting
possibilities, but I was less convinced that any of the suggested applications
were actually good ideas as-is. Like most of the work on Web3 here, this feels
very much like a hammer looking for a nail. Like, these are definitely a cool
set of ideas and I could totally believe that the &ldquo;next big thing&rdquo; will build
on top of these, but I&rsquo;m still sort of waiting for an actual concrete use-case
to use this for (that isn&rsquo;t self-referentially motivated by interoperability on
some blockchain).</p>
<h3 id="enabling-the-next-generation-of-multi-region-applications-with-cockroachdb">Enabling the Next Generation of Multi-Region Applications with CockroachDB</h3>
<p><a href="https://doi.org/10.1145/3514221.3526053">https://doi.org/10.1145/3514221.3526053</a></p>
<p>This was a neat paper about how CockroachDB built globally distributed
replication into their database. This was a very well-written paper &ndash; I
found it extremely well-motivated and easy to follow.</p>
<p>In particular, every table can be configured with different survivability
configurations (e.g. <code>SURVIVE ZONE FAILURE</code>) and locality settings (<code>GLOBAL</code>,
<code>REGIONAL BY ROW</code>, <code>REGIONAL BY TABLE</code>). Survivability settings determine how
data is replicated across nodes, while the locality setting determimnes how the
data is treated by the query optimizer (GLOBAL tables get low-latency reads
from all regions at the expense of slowe writes, while REGIONAL rows get
low-latency reads form the &ldquo;home&rdquo; region at the cost of higher-latency reads
everywhere else).</p>
<p>Obviously, building this information into the databases reduces the burden on
the application programmer, but it also allows CockroachDB to automatically
enforce &ldquo;best practices&rdquo; (e.g. enforcement of global unique constraints) and to
exploit region properties during query planning (e.g. minimizing data movement
by exploiting UNIQUE constraints). I&rsquo;m not sure they did a good job
establishing that this desired information was &ldquo;sufficient&rdquo; for all
geo-replication use-cases, but it did a good job arguing that these features
worked for at least a substantial portion of use-cases.</p>
<h3 id="formality-considered-harmful-experiences-emerging-themes-and-directions-on-the-use-of-formal-representations-in-interactive-systems">Formality Considered Harmful: Experiences, Emerging Themes, and Directions on the Use of Formal Representations in Interactive Systems</h3>
<p><a href="https://doi.org/10.1023/A:1008716330212,">https://doi.org/10.1023/A:1008716330212,</a> or accessible at <a href="https://www.csdl.tamu.edu/~shipman/papers/cscw.pdf">https://www.csdl.tamu.edu/~shipman/papers/cscw.pdf</a></p>
<p>This is an HCI paper about the mismatches between human needs and the
&ldquo;formality&rdquo; that most computer systems require. I think the key quote
here is:</p>
<blockquote>
<p>When formalisms are embedded in computer systems, users often must
engage in activities that might not ordinarily be part of their tasks:
breaking information into chunks, characterizing content with a name
or keywords, categorizing information, or specifying how pieces of
information are related.</p>
</blockquote>
<p>The paper goes through a couple examples of this &ndash; although the
specifics are fairly dated (the paper was written in 1999), analogies to
modern systems are pretty obvious (who hasn&rsquo;t dealt with &ldquo;what folder
hierarchy should I use&rdquo; or &ldquo;should I setup a tagging system&rdquo;?).</p>
<p>The paper did a good job describing the problem, but I found the suggested
solutions to be unhelpful. Basically, the suggestions boil down to &ldquo;make sure
to use the &ldquo;right&rdquo; formalisms&rdquo;, &ldquo;see if ou can make formalization incremental&rdquo;,
and &ldquo;see if you can rely on informal organizations (e.g. spatial organization,
or textual search)&rdquo;, but their examples of each were either (1) not very
detailed or (2) too dated to the 1990s for me to really understand the
suggestion. It&rsquo;d be interesting to see a refreshed version of this paper with
more modern references + examples.</p>
<h3 id="relational-e-matching">Relational E-matching</h3>
<p><a href="https://doi.org/10.1145/3498696">https://doi.org/10.1145/3498696</a></p>
<p>This is my favorite type of paper: something that applies a technique
from one field to a problem in a separate field and gets large speedups.</p>
<p>E-graphs are data structures for representing equivalence classes of
function calls, and can be used for compiler optimizations (google
equality saturation). One key part of this is &ldquo;e-matching&rdquo;, which is
essentially pattern matching on e-graphs (e.g.<code>X - X =&gt; 0</code>). This paper
makes two observations:</p>
<ol>
<li>E-graphs can be easily represented as relational databases</li>
<li>E-matching essentially reduces to conjunctive queries on these
databases.</li>
</ol>
<p>Conjunctive queries on relational databases are super well-studied, so
this opens up a bunch of techniques. In particular, the authors apply
the &ldquo;generic&rdquo; worst-case optimal join algorithm to actually solve these
conjunctive queries, which are (as the name implies) asymptotically
optimal algorithms. Unsurprisingly, this gives large speed-ups over the
previous state-of-the-art.</p>
<p>I think this paper was very well done, but I wish they went further on a
number of axes:</p>
<ul>
<li>They integrated their algorithm into an existing Rust library for
e-graphs (<a href="https://docs.rs/egg">egg</a>), but I wished they had pushed
their &ldquo;e-graphs as relational databases&rdquo; further and looked at whether
you could just use a Datalog database for storing + querying e-graphs.
There are some pretty fast + specialized Datalog engines (e.g.
Souffle), and it&rsquo;d be interesting to see how much the &ldquo;specialized
e-graph&rdquo; functionality from egg was important.</li>
<li>I wished the authors had explained more about how their joins are
actually implemented. There&rsquo;s an increasing amount of
<a href="https://doi.org/10.14778/3407790.3407797">literature</a> on the
implementation of these joins (and not just their asympotitcs), and I
suspect the authors could do even better by adopting some of these
tricks.</li>
</ul>
<h3 id="a-tree-clock-data-structure-for-causal-orderings-in-concurrent-executions">A tree clock data structure for causal orderings in concurrent executions</h3>
<p><a href="https://doi.org/10.1145/3503222.3507734">https://doi.org/10.1145/3503222.3507734</a></p>
<p>This paper introduces a &ldquo;tree clock&rdquo;, which is faster version of a
vector clock. The key idea is that tracking the &ldquo;provenance&rdquo; of
information lets you &ldquo;skip&rdquo; some updates. For example, suppose we
have the following three operatoins:</p>
<ol>
<li><span class="math inline">\(\mathbb{C}_2 = \mathbb{C}_1 \oplus \mathbb{C}_2\)</span></li>
<li><span class="math inline">\(\mathbb{C}_3 = \mathbb{C}_2 \oplus \mathbb{C}_3\)</span></li>
<li><span class="math inline">\(\mathbb{C}_3 = \mathbb{C}_1 \oplus \mathbb{C}_3\)</span></li>
</ol>
<p>Then the 3rd join operation is entirely redundant &ndash; because
<span class="math inline">\(\mathbb{C}_1\)</span> hasn&rsquo;t been updated since operation 1, any &ldquo;new&rdquo;
information that it carries must have already been transmitted to
<span class="math inline">\(\mathbb{C}_3\)</span> in step 2! A tree clock essentially takes this idea and
generalizes it to more complicated cases (essentially you form a &ldquo;tree&rdquo;
of provenances which track from which clocks you learned about which
information). Thus, while vector clock operations generally take <span class="math inline">\(O(k)\)</span>
time (where <span class="math inline">\(k\)</span> is the # of threads), tree clocks can be asymptotically
faster.</p>
<p>I thought the idea was interesting, but found this paper to be pretty
unsatisfying on a number of levels:</p>
<ul>
<li>The tree clock algorithm seems quite complicated and full of
data-dependent branching, and I suspect it has a much larger constant
factor than vector clocks even if they&rsquo;re asymptotically faster. It
would&rsquo;ve been nice to:
<ul>
<li>Explicitly benchmark the # of threads we need until tree clocks are
&ldquo;worth it&rdquo; over vector clocks.</li>
<li>Investigate whether there are any low-level tricks we can use to optimize
the tree clocks (for instnace, vector clocks can be pretty trivially SIMD
accelerated &ndash; is there any such analogue with tree clocks?).</li>
</ul>
</li>
<li>It would&rsquo;ve been good to contextualize the speed ups from tree clocks
on some real applications (especially things like Go&rsquo;s race detector where
you might plausibly have millions of go routines). How does the speedup from
the tree clocks compare to other sources of overhead in the overall
application?</li>
<li>As a style complaint, I found the paper&rsquo;s notation to be pretty hard to
follow, and there was way too much emphasis on formal definitions + theorems
without enough intuitive explanations.</li>
</ul>
<h3 id="handoff-strategies-in-settings-with-high-consequences-for-failure-lessons-for-health-care-operations">Handoff strategies in settings with high consequences for failure: lessons for health care operations</h3>
<p><a href="https://doi.org/10.1093/intqhc/mzh026">https://doi.org/10.1093/intqhc/mzh026</a></p>
<p>This paper is essentially a summary of a series of interviews for
&ldquo;handoffs&rdquo; (transitions between shifts) in &ldquo;high-stakes&rdquo; situations (e.g.
nuclear reactors, ambulance dispatches, etc) trying to synthesize &ldquo;common&rdquo;
strategies across the two. Although I found it informative, I feel like more
things would&rsquo;ve struck out at me if I was actually involved in an on-call
rotation or consequence-heavy operations. As-is, most of the suggestions seemed
pretty common-sensical and I don&rsquo;t think I really remembered much. Someone
more-in-the-trenches would probably have a better sense of what&rsquo;s actually
important / distinctive and what&rsquo;s just &ldquo;obvious&rdquo;.</p>
<h3 id="potential-outcome-and-directed-acyclic-graph-approaches-to-causality-relevance-for-empirical-practice-in-economics">Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics</h3>
<p><a href="https://doi.org/10.1257/jel.20191597">https://doi.org/10.1257/jel.20191597</a></p>
<p>This was basically an economist comparing and contrasting the Rubin
potential outcomes framework for causal inference with Pearl&rsquo;s DAG
framework. Imbens definitely comes down on the side of the potential
outcomes framework, for a couple of reasons:</p>
<ul>
<li>Although Imbens agrees that DAGs have pedagogical advantages, he
argues that in practice &ldquo;most&rdquo; causal inference is for fairly cookie
cutter problems that are sufficiently well-understood that you don&rsquo;t
need DAGs (e.g. regression discontinuities or instrumental variables)</li>
<li>DAGs have a sort-of artificial separation between &ldquo;DAG specification&rdquo;
and the actual estimation + prediction tasks. Potential outcomes,
OTOH, usually bundles these things together which makes them easier to
actually use.</li>
<li>The DAG framework doesn&rsquo;t have great techniques for &ldquo;feedback cycles&rdquo;,
where my current value influences your future value which influences
my future value.</li>
<li>Potential outcomes has a long established history in economics, while
there&rsquo;s not a great literature of DAGs being applied to real-world
problems. Pedagogically, too much of the DAG literature is about &ldquo;toy
problems&rdquo; with limited applicability to the real world.</li>
</ul>
<p>As someone who&rsquo;s never done much casual inference work and has only really
encountered the DAG framework this was a super interesting read. Of these, I
think only the 3rd is a technical weakness of DAGs (although it&rsquo;s not clear to
me how potential outcomes handles feedback cycles better) &ndash; the rest of the
criticisms seem pretty cultural. That&rsquo;s not to say they&rsquo;re illegitimate of
course &ndash; I definitely sympathize with the &ldquo;toy&rdquo; nature of most DAG
introductions (which has also matched my experience). I think point 1 might
also be an interesting commentary on &ldquo;users&rdquo; vs &ldquo;developers&rdquo; here &ndash; this
strikes me as similar to how people treat statistics &ndash; while a lot of
statisticians I know want to build fancy generative models for inference, most
people seem to get by with cookie-cutter t-tests.</p>
<p>I hope the DAG and potential outcomes communites grow closer in the future
though and harmonize on their notation + jargon &ndash; as a total outsider to
causal inference, it&rsquo;s pretty frustrating  to have to navigate 3 different
frameworks (DAGs for computer scientists, potential outcomes for economics, and
marginal structural models / G-models for epidemiology) and it&rsquo;d be great to
have some kind of paradigm unification (if only to make it easier to understand
what&rsquo;s going on).</p>
<h3 id="understanding-and-exploiting-optimal-function-inlining">Understanding and exploiting optimal function inlining</h3>
<p><a href="https://doi.org/10.1145/3503222.3507744">https://doi.org/10.1145/3503222.3507744</a></p>
<p>This was a really fun paper, with a very simple idea: brute force all
possible inlining possibilities and check how close Clang&rsquo;s <code>-Os</code> setting
gets to the smallest possible size.  Naively, if you have a call graph
of <span class="math inline">\(E\)</span> edges there are <span class="math inline">\(2^E\)</span>  possibilities which is clearly way too much
to search for even mildly complicated programs. But the key insight is
if you can &ldquo;split&rdquo; call graph into two components with a single edge
between them, then you only need to search <span class="math inline">\(2^{E-1} + 2^{E_1} + 2^{E_2} 1\)</span>
possibilities (<span class="math inline">\(2^{E-1}\)</span> possibilities if you inline that edge and
<span class="math inline">\(2^{E_1} + 2^{E_2} + 1\)</span> if you don&rsquo;t). This recursive partitioning turns
out to be &ldquo;good enough&rdquo; that you can actually search mildly complicated
programs exhaustively (the authors choose the SPEC2017 benchmark suite).</p>

</article>

    </div>
    <footer>
      <nav class="nav">
        <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/"><img class = "cc" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        <div>Inspired by the <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
      </nav>
    </footer>
  </body>
</html>
