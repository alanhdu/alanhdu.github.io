<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1" /><title> What I&#39;ve Been Reading (Sep 2022) &mdash; Amateur Hour </title>
  <meta property="og:title" content="What I&#39;ve Been Reading (Sep 2022)" />
<meta property="og:description" content="a short log of things I read last month." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://alanhdu.github.io/posts/2022-10-27-what-im-reading/" />
<meta property="article:published_time" content="2022-10-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-27T00:00:00+00:00" />

	<link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/katex.min.css" />
<script defer type="text/javascript" src="https://alanhdu.github.io/js/katex.min.js"> </script>
<script defer type="text/javascript" src="https://alanhdu.github.io/js/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement(document.body, {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false}
    ]
  });
});
</script>
<link href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.min.js" integrity="sha384-Ep9Es0VCjVn9dFeaN2uQxgGcGmG+pfZ4eBaHxUpxXDORrrVACZVOpywyzvFRGbmv" crossorigin="anonymous"></script>


  <link rel="apple-touch-icon" sizes="180x180" href="https://alanhdu.github.io/favicon//apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="https://alanhdu.github.io/favicon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="https://alanhdu.github.io/favicon/favicon-16x16.png"/ >
</head>

  <body>
    <div class="container wrapper">
      <div class="header">
	<h1 class="site-title">Amateur Hour</h1>
	<nav class="nav">
		<ul class="flat">
      <li><a href="https://alanhdu.github.io/">Home</a></li>
      <li><a href="https://alanhdu.github.io/about">About</a></li>
      <li><a href="https://alanhdu.github.io/categories">Categories</a></li>
      <li><a href="https://alanhdu.github.io/tags">Tags</a></li>
      <li><a href="https://alanhdu.github.io/index.xml">RSS Feed</a></li>
    </ul>
  </nav>
</div>

      

<div class="post-header">
  <h2 class="title">What I&#39;ve Been Reading (Sep 2022)</h2>
  <strong class="description">Or a short log of things I read last month.</strong>
  <div>
    <span class="date">Oct 27, 2022</span>.
    Filed under
    
    <span class="category"> <a href="https://alanhdu.github.io/categories/musings">Musings</a></span>
  </div>
  Tags:
    
    <a href="https://alanhdu.github.io/tags/reading">reading</a>
</div>

<article class="markdown"><aside><strong> Table of Contents </strong> <nav id="TableOfContents">
  <ul>
    <li><a href="#books">Books</a>
      <ul>
        <li><a href="#dancing-in-the-glory-of-monsters-the-collapse-of-the-congo-and-the-great-war-of-africa">Dancing in the Glory of Monsters: The Collapse of the Congo and the Great War of Africa</a></li>
        <li><a href="#breaks-of-the-game">Breaks of the Game</a></li>
        <li><a href="#dealers-of-lightning">Dealers of Lightning</a></li>
      </ul>
    </li>
    <li><a href="#papers">Papers</a>
      <ul>
        <li><a href="#graham-synchronizing-clocks-by-leveraging-local-clock-properties">Graham: Synchronizing Clocks by Leveraging Local Clock Properties</a></li>
        <li><a href="#nwgraph-a-library-of-generic-graph-algorithms-and-data-structures-in-c20">NWGraph: A Library of Generic Graph Algorithms and Data Structures in C++20</a></li>
        <li><a href="#practical-bayesian-model-evaluation-using-leave-one-out-cross-validation-and-waic">Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</a></li>
        <li><a href="#asymptotic-equivalence-of-bayes-cross-validation-and-widely-applicable-information-criterion-in-singular-learning-theory">Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></li>
        <li><a href="#know-when-to-fold-em-an-empirical-description-of-risk-management-in-public-research-funding">Know when to fold ‘em: An empirical description of risk management in public research funding</a></li>
        <li><a href="#are-you-sure-you-want-to-use-mmap-in-your-database-management-system">Are You Sure You Want to Use MMAP in Your Database Management System?</a></li>
        <li><a href="#effective-mimicry-of-beladys-min-policy">Effective Mimicry of Belady’s MIN Policy</a></li>
        <li><a href="#rethinking-beladys-algorithm-to-accommodate-prefetching">Rethinking Belady’s Algorithm to Accommodate Prefetching</a></li>
        <li><a href="#moderates">Moderates</a></li>
        <li><a href="#power-laws-in-economics-an-introduction">Power Laws in Economics: An Introduction</a></li>
        <li><a href="#war-as-an-economic-activity-in-the-long-eighteenth-century">War as an Economic Activity in the &ldquo;Long&rdquo; Eighteenth Century</a></li>
        <li><a href="#why-organizations-fail-models-and-cases">Why Organizations Fail: Models and Cases</a></li>
        <li><a href="#what-if-we-dont-pop-the-stack-the-return-of-2nd-class-values">What If We Don’t Pop the Stack? The Return of 2nd-Class Values</a></li>
      </ul>
    </li>
    <li><a href="#articles">Articles</a></li>
  </ul>
</nav> </aside>
  <h2 id="books">Books</h2>
<h3 id="dancing-in-the-glory-of-monsters-the-collapse-of-the-congo-and-the-great-war-of-africa">Dancing in the Glory of Monsters: The Collapse of the Congo and the Great War of Africa</h3>
<p>This was probably my favorite book of the year &ndash; it&rsquo;s a history of the
Congo wars, and it&rsquo;s absolutely gut-wrenching. The author doesn&rsquo;t flinch
from just how brutal this war is<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, how complicated the conflict
are (there are like 10 armies and 20 militia groups involved), how
complicit in the atrocities all the major leaders are, and how little
the West cares. I&rsquo;m sure it would&rsquo;ve been easy to try to reduce the
story into a simple &ldquo;look how savage those Africans are&rdquo;, and I
appreciated the insistence on centering the voices of the victims, on
understanding the political dynamics (and personal relationships) that
drove the conflict, and how leaders deliberately cultivate and shape
international opinion so that the West turns a blind eye.</p>
<p>It&rsquo;s written in a very journalistic style, which is bot ha strength and
a weakness &ndash; there are tons of really revealing anecdotes (e.g. Laurent
Kabila hiding Congo&rsquo;s treasury in his bathroom) that give a lot of color
to the events, but it also means some parts of the story felt a little
glossed over (e.g. what exactly is going in Angola during this time?).</p>
<p>Reading this book made me feel ashamed about how <em>little</em> I knew about
this conflict before-hand:</p>
<ul>
<li>I knew vaguely that DRC was a failed state, but I had no idea that
they were embroiled in massive war up until 2003 (and with an
unofficial war that basically continues to the present day). How I
missed the bloodiest war since World War II that happened <em>while I
was alive</em> is crazy to me.</li>
<li>I knew about the Rwandan genocide, but had no idea that it directly
spiraled into the Congo wars. Even when I learned about in an
International Humans Rights course I took in high school, we (in
retrospect) took a very US-centric view, focusing on US foreign policy
and interventionism rather than, you know, the actual people who
suffered in the massacre and the aftermath.</li>
<li>I did not know pretty much any of the names of the leaders involved in
any of the countries (with the notable exception of Robert Mugabe in
Zimbabwe because of news articles about hyperinflation). Even many of
the independence movement leaders were new to me</li>
<li>In general, I think it&rsquo;s telling that the only books I&rsquo;ve read aobut
the Congo are this and <em>King Leopold&rsquo;s Ghost</em>&hellip;</li>
</ul>
<p>Overall, one of my favorite books of the year. Would definitely
recommend it.</p>
<h3 id="breaks-of-the-game">Breaks of the Game</h3>
<p>This book is ostensibly about the Portland Trail Blazer&rsquo;s 1979-80 NBA
season, but I think it&rsquo;s really a pretty poignant description about
American culture at the time. There&rsquo;s a lot of subtle observations about
race relations in America (between white fans and black athletes,
between black athletes and white coaches, and between white athletes and
black athletes), the effect of money on the sport (e.g. the generational
divide between the &ldquo;young&rdquo; athletes making millions of dollars and the
&ldquo;older&rdquo; generation who barely made any money), and what labor rights
look like when you&rsquo;re a rich and famous athlete (e.g. why are NBA stars
so much wealthier and more famous than, say, NFL stars?).</p>
<p>Style-wise, the book&rsquo;s a little weird. It honestly feels more like an
anthology of long-form magazine articles than a single book &ndash; each
chapter can pretty much be read stand-alone (if you know the names of
everyone involved). It&rsquo;s also written very journalistically, with lots
of emphasis on (and empathy with) a handful of individual players.
There&rsquo;s one chapter that dives into Kermit Washington&rsquo;s life that I
particularly loved.</p>
<p>Even as someone who&rsquo;s not a basketball fan, I found this an enjoyable
read that I&rsquo;d recommend to anyone interest in modern-ish American culture.</p>
<h3 id="dealers-of-lightning">Dealers of Lightning</h3>
<p>This book is essentially a history of Xerox PARC, a legendary research
center that invented GUIs, the Smalltalk language, VLSI chip design, and
Ethernet (among other things). Overall, I enjoyed it even though I
didn&rsquo;t find it particularly noteworthy. Mainly, I found it a useful
counterweight to the sheer amount of hagiography that Silicon Valley has
about PARC. It might just be because my job (trying to invent a new
human interface technology with neural interfaces in service of a new
computing paradigm with AR/VR), but I meet so many people who just
totally drink the Kool-Aid about how PARC was this magical place with
brilliant research management and genius researchers who did no wrong.</p>
<p>And like, yes, there were lots of smart people there. And the research
management was undeniably effective (although I learned that PARC could
recruit such good talent because they were willing to pay top dollar at
a time when the government was cutting CS research funding). But it was
nice to learn about the office politics between research groups, of the
prima donna displays for intellectual dominance, and the groupthink that
captured PARC even in its heydey. The mythology is that all of these
problems were only about the friction between PARC and &ldquo;those corporate
suits at Xerox who just didn&rsquo;t get it&rdquo;, but this book makes it clear
that these problems all existed <em>within</em> the walls of PARC (and even
within the walls of Bob Taylors' research group) as well.</p>
<p>Honestly, this gives me more hope about my own future career. Reading
the book, PARC felt more like a research group with an unusual density
of talent and an unusually charismatic manager &ndash; something that&rsquo;s rare
to have, but not something that&rsquo;s impossible to find.</p>
<p>Overall, I enjoyed the book, but am not sure I&rsquo;d recommend it unless you&rsquo;re
interested in PARC for external reasons.</p>
<h2 id="papers">Papers</h2>
<h3 id="graham-synchronizing-clocks-by-leveraging-local-clock-properties">Graham: Synchronizing Clocks by Leveraging Local Clock Properties</h3>
<p><a href="https://www.usenix.org/conference/nsdi22/presentation/najafi">https://www.usenix.org/conference/nsdi22/presentation/najafi</a></p>
<p>This paper argues that by actually modeling out the <em>sources</em> of clock
error, you can actually get incredibly accurate clocks using comododity
hardware on Linux, which (combined with NTP) lets you get very tightly
synchronized clocks (on the order of 100 ppb, or 100 ns of drift per
second). It includes a nice description of how clocks work in modern
systems, but I was not interested in clock synchronization enough to
engage deeply with the details of their model. Still, anyone who&rsquo;s
interested in tight clock synchronization (e.g. for something like
<a href="https://cloud.google.com/spanner/docs/true-time-external-consistency">Spanner&rsquo;s
TrueTime</a>
API) might be interested in this.</p>
<h3 id="nwgraph-a-library-of-generic-graph-algorithms-and-data-structures-in-c20">NWGraph: A Library of Generic Graph Algorithms and Data Structures in C++20</h3>
<p><a href="https://drops.dagstuhl.de/opus/volltexte/2022/16259/">https://drops.dagstuhl.de/opus/volltexte/2022/16259/</a></p>
<p>I thought this was going to be about a different way to represent graph that is
generically good compared to the standard &ldquo;soup of pointers&rdquo; or &ldquo;adjacency
matrix as sparse matrix&rdquo; that I&rsquo;m used to. Instead, this was about defining an
abstract interface for graphs (roughly abstracting over adjacency lists) and
algorithm implementations that operate over that abstract interfaces. There
didn&rsquo;t seem to be anything super special about the API design &ndash;it seemed like
sort of the obvious thing to do if you want to define an abstract graph
interface (although maybe that speaks to how well-written the exposition is)?</p>
<p>It was nice to see some modern C++ things (I had no idea about
<code>std::execution</code> or <code>std::async</code>), but otherwise I didn&rsquo;t find this
paper to be that interesting. I&rsquo;m sure this was a well-engineered
library, but nothing really stuck out to me as that noteworthy.</p>
<h3 id="practical-bayesian-model-evaluation-using-leave-one-out-cross-validation-and-waic">Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</h3>
<p><a href="https://arxiv.org/pdf/1507.04544.pdf">https://arxiv.org/pdf/1507.04544.pdf</a></p>
<p>This goes through a couple different choices for how to do model
selection with Bayesian inference. The paper assumes that you are trying
to maximize the expected log predictive density, or</p>
<p><span class="math display">\[\sum_k \ln \mathbb{E}_{\theta} p(y_k | \theta)]
\]</span></p>
<p>for some new, unobserved dataset. You can&rsquo;t calculate this this
directly, but there are two &ldquo;obvious&rdquo; sort of ways to try and get at it:</p>
<ul>
<li>You could plug in your observed data as <span class="math inline">\(y\)</span> and try to penalize the
penalize the &ldquo;double-dipping&rdquo; since you also use them to fit the
posterior. The paper claims that the WAIC (the Widely Applicable
Information Criterion) is the best way to do that.</li>
<li>Alternatively, you could try to estimate it using cross validation.
This seems to do better empirically than WAIC, but is (obviously)
expensive to compute. The paper summarizes you can apply importance
sampling to efficiently approximate leave-one-out CV, and then how to
apply &ldquo;Pareto smoothing&rdquo; to your importance weights to get a lower
variance estimate (and makes sure that the variance is even finite in
the first place!).</li>
</ul>
<p>The paper generally recommends this 2nd approach (or vanilla k-fold CV
when you have data points that are correlated with each other in your
data).</p>
<h3 id="asymptotic-equivalence-of-bayes-cross-validation-and-widely-applicable-information-criterion-in-singular-learning-theory">Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</h3>
<p><a href="http://jmlr.org/papers/v11/watanabe10a.html">http://jmlr.org/papers/v11/watanabe10a.html</a></p>
<p>This paper goes through the theory of WAIC, which is defined as (all
expectations are with respect to the posterior distribution):</p>
<p><span class="math display">\[\frac{1}{n} \sum_k \ln \mathbb{E}_{\theta}[p(y_k | \theta)] - \frac{1}{n} \sum_{k=1}^n \mathrm{Var}_{\theta}[\ln p(y_k | \theta)]
\]</span></p>
<p>I quite like this definition &ndash; frankly, I think it makes a lot more
intuitive sense than the AIC (for instance). I assume in practice, you
approximate those expectations by drawing samples from the posterior.</p>
<p>The paper was quite heavy mathematically and I admit I didn&rsquo;t have the
patience to go through and understand all the proofs step-by-step.
The paper lies heavily on &ldquo;singular learning theory&rdquo; (e.g. learning
theory when the Fisher information matrix is singular), which I was
unfamiliar with. Still, even the high-level skim gave me a sense of what
this new subfield is and how the logic is put together.</p>
<h3 id="know-when-to-fold-em-an-empirical-description-of-risk-management-in-public-research-funding">Know when to fold ‘em: An empirical description of risk management in public research funding</h3>
<p><a href="https://doi.org/10.1016/j.respol.2019.103873">https://doi.org/10.1016/j.respol.2019.103873</a></p>
<p>A relatively straightforward empirical paper that examines ARPA-E. The
ARPA &ldquo;style&rdquo; of research funding gives a lot of discretion to individual
program directors, who get to decide who actually gets funding within a
program (among other things). The paper basically shows that, yes,
ARPA-E program directors <em>do</em> in fact exercise their power to:</p>
<ul>
<li>Lengthen contracts for high-performing groups</li>
<li>Directing additional money to high-performing groups</li>
<li>Renegotiate technical milestones and deliverables</li>
<li>Terminating contracts for groups who are not hitting their milestones</li>
</ul>
<p>And that research groups with &ldquo;bonus&rdquo; funding + extended contracts tend
to yield better research as far as we can tell, although I found this
claim a little iffy given the limited measurements we have of &ldquo;goodness
of research&rdquo;.</p>
<p>On one hand, this is kind of obvious, but as the paper says:</p>
<blockquote>
<p>First, we could have found instead that PDs used their options
sparingly, consistent with expectations for program managers in
traditional funding programs. Second, we could have found that PDs
chose to expand struggling projects, seeing them in greater need of
support, while cutting short projects that show signs of success and
therefore require less public support than anticipated. And third, we
could have found that PDs modified projects ineffectually, without any
discernable productivity gains.</p>
</blockquote>
<p>In general, nothing here was too surprising but nice to get confirmation
that when you give program directors flexibility to actively manage
their program, they seem to (1) both use that power and (2) use it well.</p>
<h3 id="are-you-sure-you-want-to-use-mmap-in-your-database-management-system">Are You Sure You Want to Use MMAP in Your Database Management System?</h3>
<p><a href="https://db.cs.cmu.edu/mmap-cidr2022/">https://db.cs.cmu.edu/mmap-cidr2022/</a></p>
<p>As the title implies, this paper argues that you shouldn&rsquo;t use MMAPs
when implementing a database. I&rsquo;m generally sympathetic to this argument
&ndash; I remember using MongoDB when it still ran on a mmap-based storage
engine and seeing <em>crazily</em> bad performance once your working set didn&rsquo;t
fit in RAM<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> &ndash; but found this paper a mixed bag.</p>
<p>Honestly, the most convincing section was the historical list of
databases that was the list of databases that originally used mmaps and
moved off of the (Mongo, InfluxDB, SingleStore aka MemSQL, and
apparently the LevelDB to RocksDB fork). That&rsquo;s a fairly impressive
&ldquo;here be monsters&rdquo; list.</p>
<p>On the more technical side, the authors made 4 main critiques:</p>
<ol>
<li>Transactional Safety: honestly, I don&rsquo;t understand this critique.
It&rsquo;s good to point out that mmaps need special care (since the OS can
flush dirty pages to disk at any time), but these protocols
(especially the user-space copy-on-write) don&rsquo;t strike me as that
complicated compared to implementing a full buffer manager (and if
you do MVCC you might need a separate scratch space anyways).</li>
<li>I/O stalls: basically, mmap gives very limited control over what&rsquo;s
on disk vs in-memory, so arbitrary pointer dereferences might block
on disk I/O. I had thought that <code>mlock</code> would&rsquo;ve solved the problem,
but apparently not (although I would&rsquo;ve appreciated more detail on
<em>why</em> Linux limits the amount of memory you can <code>mlock</code>).</li>
<li>Error Handling: this was something I didn&rsquo;t think about &ndash; because
every pointer access can actually go to disk, every pointer access
might generate SIGBUS / SIGSEGV errors to signal IO problems, which
makes robustness harder.</li>
<li>mmap Performance Issues (as in, problems with the implementation of
mmap), although the paper doesn&rsquo;t really make claims that these are
&ldquo;inherent&rdquo; problems vs fixable performance issues.</li>
</ol>
<p>Overall, I thought the paper was pretty good about identifying problems
w/ mmap, but didn&rsquo;t do a good job of comparing it to the alternative of
building your own buffer manager. I&rsquo;m not aware of any off-the-shelf
buffer manager you can just use, and writing a high-quality buffer
manager strikes me as fairly complicated nowadays (and a lot of the
pieces I imagine you&rsquo;d use, like <code>io_uring</code> are pretty new).</p>
<p>(This is probably an opportunity for someone, to see if they can build a
&ldquo;buffer-manager library&rdquo; that other people can use, or at least &ldquo;parts
from which you can assemble your own buffer manager&rdquo;).</p>
<p>An interesting counterpoint is from
<a href="https://ravendb.net/articles/re-are-you-sure-you-want-to-use-mmap-in-your-database-management-system">https://ravendb.net/articles/re-are-you-sure-you-want-to-use-mmap-in-your-database-management-system</a>
which mostly mirrors my takes and replies to the 4 problems with:</p>
<ol>
<li>Copy-on-write is not <em>that</em> complicated</li>
<li>Apparently you can use <code>madvise</code> with <code>MADV_WILLNEED</code> to get most of
what you want</li>
<li>On IO Errors, you should probably just crash anyways. Error handling
is basically impossible to get right anyways (see fsyncgate in
Postgres). I believe the high-level idea (I vaguely remember <code>fsync</code>
too), but I&rsquo;m (1) unconvinced that signal handlers are good ways of
doing teardowns or that (2) there aren&rsquo;t some subset of IO errors
that could be recoverable.</li>
<li>Basically just a &ldquo;not a problem in practice&rdquo;, which might be true for
them but presumably would be pretty workload specific?</li>
</ol>
<h3 id="effective-mimicry-of-beladys-min-policy">Effective Mimicry of Belady’s MIN Policy</h3>
<p><a href="https://www.cs.utexas.edu/~lin/papers/hpca22.pdf">https://www.cs.utexas.edu/~lin/papers/hpca22.pdf</a></p>
<p>This is a fun paper about implementing a hardware CPU caching algorithm
(think CPU cache). The basic idea is to try to run Bélády&rsquo;s algorithm
by trying to <em>learn</em> the &ldquo;next use time&rdquo; of every member of cache. There
are three main parts here:</p>
<ul>
<li>The &ldquo;reuse distance predictor&rdquo; (RDP), which is just a map from Program
Counters to estimated &ldquo;time to reuse&rdquo;.  In this world, time is just a
counter that increments anytime the cache is accessed. The big
innovation seems to be thta previous versions of this approach trained
a classifier of &ldquo;should evict or not&rdquo; rather than a regression (or
multi-class classifier, since time is discrete here).</li>
<li>A &ldquo;sample cache&rdquo;, which you use to actually train the reuse distance
predictor. It&rsquo;s essentially a map from memory address to (timestamp,
PC) pairs. When an address is reused, then the corresponding <span class="math inline">\(\Delta\)</span>
is used to train the reuse predictor.</li>
<li>The actual cache, which uses the RDP to simulate Belady&rsquo;s algorithm.</li>
</ul>
<p>I was a little surprised about why the RDP is only indexed by the PC and not by
the actual memory address, but after thinking about it I actually think
that makes sense &ndash; there are probably way more addresses you want to load than
PC points and b</p>
<p>I was also surprised by how <em>small</em> the performance improvements over a
simple LRU cache was &ndash; on the ballpark of 5% for a &ldquo;state of the art&rdquo;
algorithm.</p>
<p>I didn&rsquo;t know all too much about how the CPU caches worked
under-the-hood, so this was a nice read. I think the main &ldquo;missing
piece&rdquo; for me here is (1) how the different levels of the cache work
together (I would&rsquo;ve assumed that the L1/L2/L3 cache algorithms are
designed collaboratively) and (2) some insight about whta needs to
change for multi-core caching (the paper runs experiments on multi-core
workloads but doesn&rsquo;t talk about how those might influence the cache
design).</p>
<h3 id="rethinking-beladys-algorithm-to-accommodate-prefetching">Rethinking Belady’s Algorithm to Accommodate Prefetching</h3>
<p><a href="https://www.cs.utexas.edu/~lin/papers/isca18.pdf">https://www.cs.utexas.edu/~lin/papers/isca18.pdf</a></p>
<p>Another paper about hardware caches. Basically, they observe that
Bélády&rsquo;s algorithm doesn&rsquo;t take into account the difference between
loads (where you actually use the data) vs prefetches.  The only
downside of a cache miss for prefetches is extra memory traffic, so you
usually want to trade off extra cache misses for prefetches to get
get more cache hits for loads.</p>
<p>It&rsquo;s a simple idea, and one that was well-communicated. I found their
actual &ldquo;Flex-Min&rdquo; algorithm a little complicated in <em>how</em> it traded of
cache misses for prefetches (IMO the obvious thing to do would be to
just multiply prefetch&rsquo;s &ldquo;next fetch time&rdquo; by some constant so they&rsquo;re
evicted more eagerly), but overall still a good paper.</p>
<h3 id="moderates">Moderates</h3>
<p><a href="https://www.cambridge.org/core/journals/american-political-science-review/article/moderates/71A6A9BD7EC7A5C94F975703417F866F">https://www.cambridge.org/core/journals/american-political-science-review/article/moderates/71A6A9BD7EC7A5C94F975703417F866F</a></p>
<p>When you poll people&rsquo;s policy preferences and try to fit a 1-dimensional
spatial model to them, you find that a lot of people end up being
ideological moderates. But are they moderate because they actually have
middle-of-the-road views, or are they moderate because they have some
very left-learning views and some very right-learning views in ways that
&ldquo;cancel out&rdquo;.</p>
<p>This paper tries to answer this question by fitting a mixture model to
policy preference survey data with 3 components:</p>
<ol>
<li>People who fit the traditional 1-dimensional model of ideology</li>
<li>People who select choices totally randomly (e.g. since they&rsquo;re not
paying attention to the survey). That is, Pr[y = 1] = 0.5 always.</li>
<li>People who &ldquo;cluster&rdquo; into together independently of the other two
options (that is, the probability of answering &ldquo;yes&rdquo; on question Q is
some fixed <span class="math inline">\(p_Q\)</span> for everyone in this cluster).</li>
</ol>
<p>I think modeling people&rsquo;s ideology as a mixture model is clever (and I
definitely like the inclusion of bucket 2), but I&rsquo;m pretty skeptical
about this approach in other ways:</p>
<ul>
<li>I don&rsquo;t think that this 3rd bucket captures &ldquo;inconsistent&rdquo; moderates.
There&rsquo;s nothing that claims that such inconsistent moderates agree
with each other (AFAIK), and there&rsquo;s no regularization imposing
constraints on <span class="math inline">\(p_Q\)</span> such that they&rsquo;d be actually moderate.</li>
<li>They doesn&rsquo;t do a great job establishing that their model&rsquo;s actually
fitting the data reasonably. In particular, I would&rsquo;ve liked to know
why they chose only a single cluster! Would the results have changed
if we used 2 clusters? 3 clusters? A Chinese restaurnat process to
learn the # of clusters as well?</li>
<li>Why are we even fitting a mixture model? I feel like we could answer
the stated question by looking at people&rsquo;s responses <em>after</em> we fit
the 1D ideological model &ndash; their model also gives us the &ldquo;ideological
center&rdquo; of each policy position, so can&rsquo;t we directly use those to
find if people are middle-of-the-road vs ideologically extreme but
inconsistent?</li>
</ul>
<p>It was an interesting problem and an interesting approach to the
problem, but I&rsquo;m not sure I fully trust the analysis.</p>
<h3 id="power-laws-in-economics-an-introduction">Power Laws in Economics: An Introduction</h3>
<p><a href="https://www.aeaweb.org/articles?id=10.1257/jep.30.1.185">https://www.aeaweb.org/articles?id=10.1257/jep.30.1.185</a></p>
<p>This was sort of a meh paper. It basically tries to argue that power law
distributions are empirically common in economics, comes up with some
toy models for it, and concludes with implications for the broader
field.</p>
<p>The empirical part was pretty disappointing. The way it establishes
empirical power laws is to just graph things on a log-log plot, perform
a linear regression, and argue that the <span class="math inline">\(r^2\)</span> is high. This is, IMO,
pretty unconvincing &ndash; lots of non-Pareto distributions get high <span class="math inline">\(r^2\)</span>
(e.g. a log-normal distribution) if you do this and honestly the
relationships don&rsquo;t actually <em>look</em> that linear by eye (the errors
definitely have some trends within them). I&rsquo;m a little surprised this
was the extent of the analysis since I think these problems have been
well-established since at least
<a href="https://epubs.siam.org/doi/abs/10.1137/070710111">2009</a>.</p>
<p>The &ldquo;What Causes Power Laws&rdquo; section gave a couple different generating
processes and was generally well-done (although I wish it went into more
depth about the mathematical intuition). That said, I wasn&rsquo;t sure how
seriously to take it given that I was unconvinced about the <em>empirics</em>
of the power laws presented.</p>
<p>I think the last section about broader implications was the best part,
mostly because it wasn&rsquo;t really about power laws at all! It was more
concerned with arbitrary heavy-tailed distributions (where you can&rsquo;t
just average out variance / fluctuations), which seems like the concept
we actually care about.</p>
<h3 id="war-as-an-economic-activity-in-the-long-eighteenth-century">War as an Economic Activity in the &ldquo;Long&rdquo; Eighteenth Century</h3>
<p><a href="https://journals.sagepub.com/doi/abs/10.1177/084387141002200202?journalCode=ijha">https://journals.sagepub.com/doi/abs/10.1177/084387141002200202?journalCode=ijha</a></p>
<p>This is sort of a weird paper &ndash; it&rsquo;s at once a history paper about
British economic history, a position paper for other historians to &ldquo;take
war&rsquo;s effect on economic development seriously&rdquo;, and a bit of a polemic
dunking on other economic fields. I&rsquo;m not an expert enough in British
economic history to judge the validity of its main claims, but it seemed
plausible to me baesd on other things I know. If I had to summarize the
argument, I&rsquo;d say it was that:</p>
<ul>
<li>18th century Britain should be understood as a &ldquo;fiscal-military
state&rdquo; &ndash; an organization built to raise money in order to fight wars.</li>
<li>Britain devoted much more of its economy and manpower to war
compared to its European rivals (it didn&rsquo;t &ldquo;save&rdquo; on war because it
was protected by the ocean)</li>
<li>Britain&rsquo;s huge war expenditures <em>caused</em> its huge economic boom, even
before the &ldquo;mineral&rdquo; economy of the industrial revolution. The paper
speculates that this was because of the British spending on the navy
(vs the army for other European countries).</li>
</ul>
<h3 id="why-organizations-fail-models-and-cases">Why Organizations Fail: Models and Cases</h3>
<p><a href="https://www.aeaweb.org/articles?id=10.1257/jel.54.1.137">https://www.aeaweb.org/articles?id=10.1257/jel.54.1.137</a></p>
<p>This was a nice review article chronicling a couple different
organizational failure modes along with some crisp toy models to explain
the main intuition. It didn&rsquo;t really delve into &ldquo;which of these failure
modes actually happen&rdquo; (relying more on &ldquo;empirical sketches&rdquo; than
full-throated anlysis), but I found the models to be generally well done
(although some of them were a little too artificial for my tastes).</p>
<h3 id="what-if-we-dont-pop-the-stack-the-return-of-2nd-class-values">What If We Don’t Pop the Stack? The Return of 2nd-Class Values</h3>
<p><a href="https://drops.dagstuhl.de/opus/volltexte/2022/16243">https://drops.dagstuhl.de/opus/volltexte/2022/16243</a></p>
<p>The paper basically asks whether we can return variably-sized data from
functions without having to heap allocate them (e.g. arrays of dynamic
size, or objects that satisfy an interface, or closures). They show that
yes, you can. The main idea is to <em>not</em> fully pop the stack when you
return &ndash; this is ok, as long as you pop off multiple stack frames at
once later to make up for it.</p>
<p>To do this safety, they introduce a typing discipline that enforces
&ldquo;safe stack pop deferring&rdquo;. I admit I didn&rsquo;t really follow the details
of how this type system works, but it was a neat idea!</p>
<h2 id="articles">Articles</h2>
<ul>
<li><a href="https://charity.wtf/2019/02/05/logs-vs-structured-events/">Logs vs Structured Events</a>:
This article basically advocates for &ldquo;canonical&rdquo; structured events (a
la <a href="https://stripe.com/blog/canonical-log-lines">canonical log lines</a>)
&ndash; that is, a single structured event that is emitted per request.
This event is (essentially) a &ldquo;wide&rdquo; JSON blob with potentially 100s
of fields (things like &ldquo;git commit of service&rdquo; or &ldquo;host IP address&rdquo;)
to help you retroactively figure out what happened. Because you only
emit 1 event per request, adding new fields is cheap (and you should
reduce storage costs by dynamically sampling events instead of
discarding fields).</li>
<li><a href="https://earthly.dev/blog/bazel-build/">When to use Bazel</a>: a nice
experience report about people moving to Bazel. I think the TLDR is
that moving to Bazel&rsquo;s a big change, but it does deliver on its
promises on fast + correct builds. It&rsquo;s definitely worth it when
you&rsquo;re big enough, but for smaller shops it&rsquo;s unclear how important
that is.</li>
<li><a href="https://constructionphysics.substack.com/p/where-do-economies-of-scale-come">Where do Economies of Scale Come
From</a>:
nothing here is particularly shocking, but it&rsquo;s a nice succinct
writeup of exactly what the title says.</li>
<li><a href="https://www.inkandswitch.com/cambria/">Project Cambria</a>: this details
a system for decentralized schema migrations. Basically, you create
&ldquo;lenses&rdquo; which are automated ways to go to/from schema A to schema B
and vice-versa. You can then create a graph between all the schemas
and use lenses to traverse this graph. A neat idea, and their
prototype system seemed pretty cool (e.g. a declarative YAML for
defining these lenses and then auto-generate the code, the graph, and
types for TypeScript). As the authors note, there are still some
challenges (no silver bullet!), but it&rsquo;s still a clever idea.</li>
<li><a href="https://review.firstround.com/how-superhuman-built-an-engine-to-find-product-market-fit">How Superhuman Built an Engine to Find Product Market Fit</a>:
This is one of those typical business think pieces, full of
self-aggrandizement and fluff without much actual intellectual rigor.
The core proposal is an &ldquo;algorithm&rdquo; for knowing how good your product
is when you&rsquo;re in some kind of beta mode and can&rsquo;t rely
on actual sales data. Basically, you divide customers (via survey)
into those who would be &ldquo;very disappointed&rdquo;, &ldquo;somewhat disappointed&rdquo;,
and &ldquo;not very disappointed&rdquo; if your product disappeared tomorrow. You
then (1) look at common characteristics of the &ldquo;very disappointed&rdquo;
customers to define your target market and further target your
analysis, (2) identify core value propositions among those &ldquo;very
disappointed&rdquo; users to prioritize and avoid regressing, and (3) ask
for pain points from the &ldquo;somewhat disappointed&rdquo; users to prioritize
feature work (hoping to convert them to &ldquo;very disappointed&rdquo; users).
Certainly seems plausible, but I don&rsquo;t know why they needed so many
words to convey this idea.</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>One of the most nauseating elements was a description of the
aftermath of a church massacre. After killing the villagers, the
soldiers then basically plaid human origami with the corpses &ndash; I
vividly remember the example of a corpse who had slits cut into their
belly and had their hands inserted into their own insides (like
pockets). And the reaction of the survivors was also wrenching &ndash;
something to the effect of &ldquo;I thought we were desensitized to the
violence, but this was too much&rdquo;. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Although looking back, I&rsquo;m not sure I can blame mmap for this design
choice vs Mongo&rsquo;s rather&hellip; subpar implementation quality more generally.
These were the days of the giant collection-level lock (which I&rsquo;m told was an
imporvement over the global database-level lock that preceded it)! <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</article>

    </div>
    <footer>
      <nav class="nav">
        <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/"><img class = "cc" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        <div>Inspired by the <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
      </nav>
    </footer>
  </body>
</html>
