<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1" /><title> Evaluation in Deep Learning &mdash; Amateur Hour </title>
  <meta property="og:title" content="Evaluation in Deep Learning" />
<meta property="og:description" content="use some statistics" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://alanhdu.github.io/posts/2022-10-30-ml-robustness/" />
<meta property="article:published_time" content="2022-10-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-30T00:00:00+00:00" />

	<link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/main.css" />

  <link rel="apple-touch-icon" sizes="180x180" href="https://alanhdu.github.io/favicon//apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="https://alanhdu.github.io/favicon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="https://alanhdu.github.io/favicon/favicon-16x16.png"/ >
</head>

  <body>
    <div class="container wrapper">
      <div class="header">
	<h1 class="site-title">Amateur Hour</h1>
	<nav class="nav">
		<ul class="flat">
      <li><a href="https://alanhdu.github.io/">Home</a></li>
      <li><a href="https://alanhdu.github.io/about">About</a></li>
      <li><a href="https://alanhdu.github.io/categories">Categories</a></li>
      <li><a href="https://alanhdu.github.io/tags">Tags</a></li>
      <li><a href="https://alanhdu.github.io/index.xml">RSS Feed</a></li>
    </ul>
  </nav>
</div>

      

<div class="post-header">
  <h2 class="title">Evaluation in Deep Learning</h2>
  <strong class="description">Or use some statistics</strong>
  <div>
    <span class="date">Oct 30, 2022</span>.
    Filed under
    
    <span class="category"> <a href="https://alanhdu.github.io/categories/technical">Technical</a></span>
  </div>
  Tags:
    
    <a href="https://alanhdu.github.io/tags/machine-learning">machine learning</a>, 
    <a href="https://alanhdu.github.io/tags/research">research</a>
</div>

<article class="markdown">
  <p>Over time, I&rsquo;ve found that a lot of deep learning research is a mirage. While
I&rsquo;ve usually (but not always!) been able to reproduce the authors' results on
their chosen benchmarks, the performance gains often disappear if I choose
other benchmarks to train/test against. This gets even worse when I try to apply
their results to <em>my</em> problems, which is an extremely different data domain
(<a href="https://en.wikipedia.org/wiki/Electromyography">surface electromyography</a>,
than the standard vision/text/speech that most people work with. Even in my own
research, I&rsquo;ve found that deep learning is shockingly finicky and that very
minor changes sometimes make surprisingly large changes to the benchmark
results.</p>
<p>Because of this, I&rsquo;ve come to believe that the standard deep learning
evaluation methodology is flawed. In general, the best practice seems to
be:</p>
<ol>
<li>Decide on the question we should investigate (e.g. does my fancy new
architecture work well?)</li>
<li>Pick some kind of test set to benchmark on (either some standard
benchmark like ImageNet or LibriSpeech, or some test set for your
particular application).</li>
<li>Based on your question, generate some models you want to evaluate</li>
<li>Benchmark your models on the benchmark and report the results</li>
</ol>
<p>Papers will usually include some &ldquo;intuition-building&rdquo; results &ndash; small
theorems to prove some behavior, examinations of particular data points
in the test set, or &ldquo;artificial&rdquo; benchmarks to demonstrate a specific
behavior &ndash; but I&rsquo;ve found these to be mostly about building intuition
and less about answering the &ldquo;should you use XYZ or not&rdquo;. At its core,
deep learning is very benchmark driven.</p>
<p>I think the core problem is the mismatch between step 3 and step 1 &ndash;
training a model requires choosing all kinds of nuisance parameters that
are unrelated to our research question. If I want to test whether
a Transformer outperforms an RNN (for example),  then I have to also
decide on a huge range of other things like &ldquo;do I use Adam or SGD w/
Momenutm?&rdquo; or &ldquo;what data augmentation do I use?&rdquo; or &ldquo;how many parameters
should I use in my model?&quot;<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>This impedance mismatch between evaluations on modeling <em>artifacts</em> vs
questions about modeling <em>strategies</em> has caused me to re-evaluate my
own standards for evaluation at work in two ways:</p>
<ul>
<li>At the most mechanical level, generating models is inherently
stochastic, and so any good benchmark number could just be the result
of a &ldquo;lucky&rdquo; random number generator. Because of this, I&rsquo;ve started
reporting confidence intervals and ANOVA results instead of reporting
a single benchmark number.</li>
<li>More generally, I deliberately design my experiments to establish some
kind of &ldquo;reasonable robustness&rdquo; to these nuisance parameters. That
way, if those results change in the future, my results will
(hopefully) still be valid.</li>
</ul>
<p>I&rsquo;ll tackle each of these in turn.</p>
<h2 id="robustness-to-random-seeds">Robustness to Random Seeds</h2>
<p>Given our software stacks, deep learning is almost an inherently
stochastic process. Even assuming you exactly fix all the
hyperparameters of your model, there is still inherent randomness in:</p>
<ol>
<li>The weights you initialize your model with</li>
<li>The batch sampling order</li>
<li>The data augmentation randomness (and in things like dropout)</li>
<li>The inherent non-determinism from CUDA (you could use deterministic
matmuls, but are you really going to make training slower?)</li>
<li>Scheduling randomness (e.g. for things like Hogwild or asynchronous SGD).</li>
<li>The train/validation split you use (if you&rsquo;re doing early stopping).</li>
<li>The hyperparameter search random seed (if you&rsquo;re doing HP search).</li>
</ol>
<p>These sources of randomness might seem small, but they all add up. I&rsquo;ve
seen a surprising number of results where just changing the random seed
<em>reverses</em> the results of which modeling strategy is better. If your
results are dependent on the exact random seed you use, how confident
are you that they&rsquo;re actually true?</p>
<p>There&rsquo;s a simple solution to this: treat all of this &ldquo;random
seed variation&rdquo;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> as i.i.d random noise and treat each model as coming
from some underlying distribution. That lets you do some statistics to
take into account this randomness.</p>
<p>Luckily, the majority of ML metrics are averages across test samples &ndash;
things like accuracy, word error rate, AUC, etc. So by the central limit
theorem, that underlying distribution will be very close to Gaussian,
assuming your test set is reasonably large.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> That means you
only need 2 or 3 samples to satisfy the assumptions of a Student&rsquo;s t
distribution. You might not get a lot of statistical power, but frankly
if the improvement from your modeling strategy isn&rsquo;t many times larger
than the standard error here, you should probably rethink how
practically relevant your result will be anyways.</p>
<h2 id="robustness-to-nuisance-parameters">Robustness to Nuisance Parameters</h2>
<p>After establishing the our results aren&rsquo;t just due to random chance, the
thing I try to establish is that our results are &ldquo;reasonably&rdquo; robust to
the exact modeling process we use. After all, I&rsquo;m not the only one
working on the model &ndash; while I&rsquo;m investigating the model architecture,
one of my colleagues might be introducing a new data augmentation or an
auxiliary loss function or some other improvement that&rsquo;s orthogonal to
my own. It would be nice to know that our improvements will stack
together and not just disappear once we bring them together.</p>
<p>I haven&rsquo;t found a silver bullet for this (unfortunately), but I think
you can go a long way by just thinking through your experimental design.
Lots of other domains have this kind of &ldquo;generalizability&rdquo; question,
so this isn&rsquo;t in any way unique to ML.</p>
<ul>
<li>For some nuisance parameters, I just shove them into an automated HP
search algorithm &ndash; that way, I can just use the same statistical
tests to get a handle on how much the HPs influence the model
performance.
<ul>
<li>For some HPs this can very expensive (e.g. a Bayesian hyperparameter
search that ends up fitting many models within it). To make this
cheaper, sometimes I&rsquo;ll just randomly generate hyperparameters
without any search and directly compare the distribution of
benchmark outputs. Once you have this distribution, you can also
&ldquo;simulate&rdquo; a random search by bootstrapping the max value of a
fixed-sized subsample.</li>
</ul>
</li>
<li>For other nuisance parameters, I try to choose some qualitatively
different &ldquo;parameter regimes&rdquo; to search through (e.g. for data
augmentation try to fit a ConvNet, a RNN, and a transformer, or try
different amounts of data sizes to check for scaling, or different
orders of magnitude for the # of parameters). If I get the same result
across several different parameter choices, then I can be more confident that
the result will generalize to <em>other</em> parameter choices.
<ul>
<li>The number of possible parameter choices usually very high, so
careful experimental design to subsample the space can be very
helpful (e.g. using paired comparisons for better statistical power,
or using a latin square design)</li>
<li>At my most ambitious, I try to show that the same technique works
across multiple different applications that we&rsquo;re working on &ndash; that
helps show that the technique captures something generalizable about
sEMG and our general porblem domain and not just the details of my
specific prolbem. This can be a lot of work though, since different
applications can look quite different (and often have fairly
different codepaths for training models).</li>
</ul>
</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Of course, what&rsquo;s a nuisance parameter depends on the exact
question we want answered: while for some questions &ldquo;what optimizer
should I use&rdquo; isn&rsquo;t that relevant, for other questions it might be
crucial. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Yes, I know that technically they&rsquo;re not all controllable by
random seeds, but the name still fits. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Technically you also need to establish that your variable
has finite variance, but most metrics are inherently bounded. The only
time I&rsquo;ve personally seen this pop-up has been when doing some
Bayesian modeling and trying to use the expected log predictive
density, which could potentially have unbounded variance. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</article>

    </div>
    <footer>
      <nav class="nav">
        <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/"><img class = "cc" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        <div>Inspired by the <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
      </nav>
    </footer>
  </body>
</html>
