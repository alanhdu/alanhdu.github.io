<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1" /><title> Nice bounds on black box entropy estimators &mdash; Amateur Hour </title>
  <meta property="og:title" content="Nice bounds on black box entropy estimators" />
<meta property="og:description" content="a couple of helpful derivations" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://alanhdu.github.io/posts/2020-03-20-entropy-estimation/" />
<meta property="article:published_time" content="2020-03-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-03-20T00:00:00+00:00" />

	<link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://alanhdu.github.io/css/katex.min.css" />
<script defer type="text/javascript" src="https://alanhdu.github.io/js/katex.min.js"> </script>
<script defer type="text/javascript" src="https://alanhdu.github.io/js/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement(document.body, {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false}
    ]
  });
});
</script>
<link href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.min.js" integrity="sha384-Ep9Es0VCjVn9dFeaN2uQxgGcGmG+pfZ4eBaHxUpxXDORrrVACZVOpywyzvFRGbmv" crossorigin="anonymous"></script>


  <link rel="apple-touch-icon" sizes="180x180" href="https://alanhdu.github.io/favicon//apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="https://alanhdu.github.io/favicon/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="https://alanhdu.github.io/favicon/favicon-16x16.png"/ >
</head>

  <body>
    <div class="container wrapper">
      <div class="header">
	<h1 class="site-title">Amateur Hour</h1>
	<nav class="nav">
		<ul class="flat">
      <li><a href="https://alanhdu.github.io/">Home</a></li>
      <li><a href="https://alanhdu.github.io/about">About</a></li>
      <li><a href="https://alanhdu.github.io/categories">Categories</a></li>
      <li><a href="https://alanhdu.github.io/tags">Tags</a></li>
      <li><a href="https://alanhdu.github.io/index.xml">RSS Feed</a></li>
    </ul>
  </nav>
</div>

      

<div class="post-header">
  <h2 class="title">Nice bounds on black box entropy estimators</h2>
  <strong class="description">Or a couple of helpful derivations</strong>
  <div>
    <span class="date">Mar 20, 2020</span>.
    Filed under
    
    <span class="category"> <a href="https://alanhdu.github.io/categories/technical">Technical</a></span>
  </div>
  Tags:
    
    <a href="https://alanhdu.github.io/tags/math">math</a>, 
    <a href="https://alanhdu.github.io/tags/machine-learning">machine learning</a>, 
    <a href="https://alanhdu.github.io/tags/information-theory">information theory</a>
</div>

<article class="markdown">
  <p><strong>TLDR</strong>: A brief summary of a <a href="https://arxiv.org/abs/1811.04251">paper</a>
that I read recently about why it&rsquo;s hard to estimate information
entropy.</p>
<p>I recently came across a paper with some proofs of the number of
samples you need to accurately estimate various information theoretic
quantities in a &ldquo;black box&rdquo; way.  Although I enjoyed the proofs, I found
hard to understand as-written, and so I thought I&rsquo;d post a more
understandable explanation of the core results.</p>
<p>Suppose we have some variable <span class="math inline">\(X\)</span> that we can draw i.i.d samples from
but know nothing else about. Then how many samples do we need to draw to
accurately estimate its entropy <span class="math inline">\(H(X)\)</span>, its KL-divergence from another
random variable <span class="math inline">\(D(Y \parallel X)\)</span>, or its mutual information with
another random variable <span class="math inline">\(I(X; Y)\)</span>? We will show that for our estimate
to be &ldquo;accurate with high probability&rdquo;, then the number of samples we
require is exponential in the quantity we&rsquo;re estimating.</p>
<p>I&rsquo;ll present the arguments from the paper fairly informally (in
particular, I won&rsquo;t actually define what I mean by &ldquo;high probability&rdquo;,
although it shouldn&rsquo;t be too hard to formalize the notion that I&rsquo;m
using). The core argument is fairly straightforward and I think the
formalization doesn&rsquo;t add much to understanding the underlying logic.</p>
<h2 id="core-argument">Core Argument</h2>
<p>We&rsquo;ll use the same generic argument for all three cases. Suppose we want
to estimate <span class="math inline">\(f(X)\)</span> in a black box way, where our only information from
<span class="math inline">\(X\)</span> comes from drawing <span class="math inline">\(N\)</span> i.i.d samples. We will first observe that
if our estimator <span class="math inline">\(E\)</span> is within <span class="math inline">\(f(X) \pm \epsilon\)</span> with &ldquo;high
probability&rdquo; (i.e. &ldquo;accurate with high probability&rdquo;), then <span class="math inline">\(E - \epsilon < f(X)\)</span> with &ldquo;high probability&rdquo;, giving us a &ldquo;black-box lower
bound&rdquo; estimator that is correct with &ldquo;high probability&rdquo;.</p>
<p>But we&rsquo;ll show that for any black box lower-bound estimate of <span class="math inline">\(f(P)\)</span>
that is correct with &ldquo;high-probability&rdquo;, our lower-bound estimate will
be smaller than some function <span class="math inline">\(B(N)\)</span> with non-negligible probability.
That implies that <span class="math inline">\(E - \epsilon < B(N)\)</span>, or <span class="math inline">\(E < B(N) + \epsilon\)</span>
with non-negligible probability.</p>
<p>Thus for <span class="math inline">\(E\)</span> to be a &ldquo;accurate with high probability&rdquo;, we need we need
<span class="math inline">\(B(N) + \epsilon \ge f(X) - \epsilon\)</span>. This will force us to conclude
that <span class="math inline">\(N \ge B^{-1}(f(X) - 2\epsilon)\)</span> for <span class="math inline">\(E\)</span> to be accurate with high
probability.</p>
<p>In other words, if we can show that any black-box lower-bound estimate
of <span class="math inline">\(f(P)\)</span> that is correct with high probability must be <span class="math inline">\(\le B(N)\)</span>
with non-negligible probability, then we can conclude that for our
black-box estimator to be accurate with high probability, we need <span class="math inline">\(N > B^{-1}(f(X) - 2\epsilon)\)</span>.</p>
<h2 id="kullback-leibler-divergence-estimation">Kullback-Leibler Divergence Estimation</h2>
<p>We&rsquo;ll start with the black-box lower bound for Kullback-Leibler
divergence (this is section 3 of the paper), which IMO is the most
straightforward of the three estimators.</p>
<p>Suppose we want to lower-bound the KL-divergence of two random variables
<span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> in a &ldquo;black box&rdquo; way, where our only information for
<span class="math inline">\(Q\)</span> comes from drawing i.i.d samples (we can assume we know the exact
pmf/pdf of <span class="math inline">\(P\)</span>).</p>
<p>Now, let us define a new random variable <span class="math inline">\(\tilde{Q}\)</span> that we generate
from a two-step process: first, we flip a coin that is heads with
probability <span class="math inline">\(\frac{1}{N}\)</span>. If we get heads, then we sample
<span class="math inline">\(\tilde{Q}\)</span> from <span class="math inline">\(P\)</span>. Otherwise, we sample <span class="math inline">\(\tilde{Q}\)</span> from <span class="math inline">\(Q\)</span>.</p>
<p>Now, if we denote the pmf of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> by <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>
respectively, then by construction we have that the pmf of <span class="math inline">\(\tilde{Q}\)</span>
is <span class="math inline">\(\tilde{q}(x) = \frac{(N-1)q(x) + p(x)}{N}\)</span>, which implies that:</p>
<p><span class="math display">\[\begin{aligned}
D(P \parallel \tilde{Q}) &= \sum p(x) \lg \frac{p(x)}{\tilde{q}(x)} \\
&= \sum p(x) \lg \frac{N p(x)}{(N-1)p(x) + q(x)} \\
&= \lg N + \sum p(x) \lg \frac{p(x)}{(N-1)p(x) + q(x)} \\
&\le \lg N
\end{aligned}
\]</span></p>
<p>(using the fact that <span class="math inline">\( p(x) \le (N-1)p(x) + q(x)\)</span> for <span class="math inline">\(N \ge 2\)</span>, and
so <span class="math inline">\( \lg \frac{p(x)}{(N-1)p(x) + q(x)} \le 0\)</span>).</p>
<p>But with probability <span class="math inline">\((1 - \frac{1}{N})^N \ge \frac{1}{4}\)</span>, any sample
of <span class="math inline">\(N\)</span> draws from <span class="math inline">\(\tilde{Q}\)</span> will be indistinguishable from a
sample from <span class="math inline">\(Q\)</span>.</p>
<p>Thus, any lower bound estimator for <span class="math inline">\(D(P \parallel Q)\)</span> that is
accurate with &ldquo;high probability&rdquo; must also be <span class="math inline">\(\le \lg N\)</span> with
non-negligible probability.</p>
<p>Applying our core argument, we thus have that any accurate black-box
estimator of <span class="math inline">\(D(P \parallel Q)\)</span> requires <span class="math inline">\(N > 2^{D(P \parallel Q) - 2\epsilon}\)</span> samples, which is exponential in <span class="math inline">\(D(P \parallel Q)\)</span>.</p>
<p>Note that this argument generalizes perfectly well to continuous <span class="math inline">\(Q\)</span>
as well: just replace pmf with pdf and replace our sums with integrals
and the proofs naturally follow.</p>
<h2 id="bounds-of-discrete-entropy-estimation">Bounds of Discrete Entropy Estimation</h2>
<p>We can use a similar trick (replacing a random variable with another
adversarially chosen random variable that is &ldquo;indistinguishable&rdquo; with
high probability) to also derive some bounds on sample sizes for
black-box entropy estimation.</p>
<p>Suppose we have some discrete random variable <span class="math inline">\(P\)</span> with pmf <span class="math inline">\(p(x)\)</span>.
to lower-bound <span class="math inline">\(H(P)\)</span> with high probability using a black-box
estimator.  We know that entropy is only concerned with the probability
mass function <span class="math inline">\(p(x)\)</span>: the actual identity of events in our sample
space is irrelevant. That means given a random sample i.i.d in <span class="math inline">\(P\)</span>,
the the <strong>frequency of frequencies vector</strong> is a sufficient statistic
for estimating information entropy, where we define the frequency of
frequencies vector <span class="math inline">\(N\)</span> as the vector where <span class="math inline">\(N_r\)</span> records the number
of distinct elements of <span class="math inline">\(P\)</span>&rsquo;s support that we see <span class="math inline">\(r\)</span> times in our
sample (you might recognize this from <a href="https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation">Good-Turing frequency
estimation</a>.
The frequency of frequencies vector stores exactly the information that
we need for entropy (frequencies) while discarding the actual identity
of the objects that we observed in our sample.</p>
<p>We will show that for such black-box lower-bound estimators, we have <span class="math inline">\(E \le \lg (2N^2) = 2 \lg N + 1\)</span> with non-negligible probability. To do
so, let&rsquo;s only consider <span class="math inline">\(P\)</span> with at least <span class="math inline">\(2N^2\)</span> elements in its
support (if <span class="math inline">\(P\)</span>&rsquo;s support is smaller, then we trivially have that
<span class="math inline">\(H(P) < \lg 2N^2\)</span>, and so any lower bound of <span class="math inline">\(H(P)\)</span> is also <span class="math inline">\(\le \lg2N^2\)</span>).</p>
<p>For such <span class="math inline">\(P\)</span>, let <span class="math inline">\(\Omega\)</span> denote <span class="math inline">\(P\)</span>&rsquo;s support, and let <span class="math inline">\(A\)</span> be
the set of <span class="math inline">\(N^2\)</span> most likely outcomes from <span class="math inline">\(P\)</span> and let <span class="math inline">\(B\)</span> be the
set of the next <span class="math inline">\(N^2\)</span> most likely outcomes (breaking ties
arbitrarily).  We will construct a new random variable <span class="math inline">\(\tilde{P}\)</span> by
first drawing a random sample from <span class="math inline">\(P\)</span>: if our sample is <span class="math inline">\(\in A\)</span>,
then we will return that sample. Otherwise, we will draw a sample
uniformly from <span class="math inline">\(B\)</span>  In other words, this random variable has pmf:</p>
<p><span class="math display">\[\tilde{p}(x) = \begin{cases}
p(x) ,& x \in A \\
\frac{1}{N^2} \Pr [ P \not\in A ] ,& x \in B \\
0 ,&  x \not \in A \land x \not \in B
\end{cases}
\]</span></p>
<p>Because the support of <span class="math inline">\(\tilde{P}\)</span> has size <span class="math inline">\(2N^2\)</span>, we have that
<span class="math inline">\(H(\tilde{P}) \le \lg 2N^2\)</span> trivially.</p>
<p>We will now argue that i.i.d samples from <span class="math inline">\(P\)</span> are indistinguishable to
our entropy estimator from samples in <span class="math inline">\(\tilde{P}\)</span> as long as we have
no element in <span class="math inline">\(x \not \in A\)</span> that are sampled more than once. If this
happens, then the entropy estimator just sees a frequency-of-frequency
vector from the samples in <span class="math inline">\(A\)</span> (which by construction have the same
distributions for both <span class="math inline">\(P\)</span> and <span class="math inline">\(\tilde{P}\)</span>) along with a bunch of
singleton samples from <span class="math inline">\(\Omega - A\)</span> (which are also indistinguishable
between <span class="math inline">\(P\)</span> and <span class="math inline">\(\tilde{P}\)</span>, because we are discarding their
identities).</p>
<p>The probability of having no collisions in <span class="math inline">\(\Omega - A\)</span> also must
happen with non-negligible probability: suppose we draw a i.i.d sample
of size <span class="math inline">\(N\)</span> from either <span class="math inline">\(\tilde{P}\)</span> or <span class="math inline">\(P\)</span>. Then we can upper
bound the probability of a collision in <span class="math inline">\(\Omega - A \supseteq B \)</span> with
the probability of a collision from samples frawn uniformly over <span class="math inline">\(B\)</span>
because <span class="math inline">\(\forall x \in B, \frac{1}{N^2} \ge p(x) \land \tilde{p}(x)\)</span>.
The probability of having no such collisions is thus lower-bounded for
both <span class="math inline">\(P\)</span> and <span class="math inline">\(\tilde{P}\)</span> by:</p>
<p><span class="math display">\[\prod_{k=0}^{N-1} (1 - \frac{k}{N^2}) \ge (1 - \frac{1}{N})^N \ge \frac{1}{4}
\]</span></p>
<p>and so such collision-free samples happen with non-negligible
probability. Thus, we have shown that any accurate black-box estimator
of <span class="math inline">\(H(P)\)</span> must be <span class="math inline">\(\le 2 \lg N + 1\)</span> with non-negligible probability.</p>
<p>Using our core argument, we thus have for any
accurate-with-high-probabiity black box estimator of <span class="math inline">\(H(P)\)</span>, we need
<span class="math inline">\(N \ge 2^{\frac{H(P) - 1}{2} - \epsilon} = \Omega(2^{-\epsilon}\sqrt{2^{H(P)}})\)</span>.</p>
<h2 id="bounds-on-mutual-information">Bounds on Mutual Information</h2>
<p>We know that for any two random variables <span class="math inline">\(X, Y\)</span>, we have that <span class="math inline">\(I(X; Y) = H(X) - H(X|Y) \le H(X)\)</span>. Thus, any lower bound on <span class="math inline">\(I(X; Y)\)</span> is
also a lower bound on <span class="math inline">\(H(X)\)</span>, and so any black box lower bound
estimator of <span class="math inline">\(I(X; Y)\)</span>  must also be <span class="math inline">\(\le 1 + 2 \lg N\)</span> with
non-negligible probability. We thus have the same bound that any
accurate-with-high-probability black box estimator of <span class="math inline">\(I(X;Y)\)</span> needs
<span class="math inline">\(N = \Omega(2^{-\epsilon}\sqrt{2^{H(P)}})\)</span> samples.</p>
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>I enjoyed these proofs because they were relatively straightforward: the
key insights are about generating the right adversarial distributions to
establish bounds on lower-bounds, but once you get the right adversary
the mathematics aren&rsquo;t particularly complicated.</p>
<p>One interesting point is that these are exponential bounds in the
quantity itself: that is, if we want to estimate the entropy of a
high-entropy distribution, this implies we need lots of samples. But if
we want to estimate low-entropy distributions, the mutual information of
mostly independent random variables, or KL-divergences between almost
identical random variables, then these bounds don&rsquo;t tell us all that
much.</p>

</article>

    </div>
    <footer>
      <nav class="nav">
        <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/"><img class = "cc" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        <div>Inspired by the <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
      </nav>
    </footer>
  </body>
</html>
